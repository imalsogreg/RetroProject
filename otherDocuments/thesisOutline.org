
\newpage

* Overview

Three chapters about populations of neurons in the hippocampus
coordinating their activity on fine timescales, and how that
coordination relates to the larger context of hippocampal inputs,
outputs, and experimental measurements. In the first chapter, explore
the lack of synchrony in timing of the brain rhythms underlying
excitatory drive in hippocampal place cells, and the impact of those
differences on the ability of distant place cells to coordinate reliably
during population-wide coding events. In the second chapter, describe
the development of a tool for detecting place cell population coding
events and using the information they contain to manipulate brain
activity or behavior experimentally. In the third chapter, describe the
unexpected transition of retrosplenial cortex into a slow-wave-sleep
like state during reward consumption, and the timing relationship
between hippocampal activity and retrosplenial cortex during these
events. Conclude with a discussion about coordinated encoding that ties
the three chapters together - how does the sequential nature of coding
in hippocampus relate to coding in the afferents and efferents - 
do they also encode information in spike sequences? What experiments 
using real-time ensemble decoding would help answer those questions?

\newpage

* General Background

** Hippocampal anatomy and physiology relevant to information coding

A brief reveiw of cellular organization in the hippocampus is given to help the reader stay oriented during discussions of electrode placement and traveling wave propagation. We also describe the freely moving rat's local field potential signatures and single-unit spiking properties, which are central to the rest of the thesis.

*** Hippocampal anatomy: cell layer and dendritic layers
The rat hippocampus is a curved, complex, three-dimensional structure most easily thought of as a sheet, about 10 by 7 mm face-on and 1mm thick, folded into a 'C' shape first along its short axis, and again into a larger 'C' along its long axis. The face of the sheet is fully tiled by primarily excitatory pyramidal neurons. Their cell bodies of these neurons are collected into a thin (about 0.1mm) band whithin the sheet's 1mm thickness. 

Basal dendritic arbors extending upward from the cell bodies toward the skull (the hippocampus is inverted relative to cortex) for 0.25mm form the /stratum oriens/. Apical dendrites extend downward for 0.5mm forming the /stratum radiatum/, and then branch widely to form the /stratum laconosum moleculare/.

After folding and curling, the far end along the longer dimension of the sheet terminates near the septal nuclei, and the other travels backward and bends down to embed itself in temporal cortex. The long axis of the hippocampus is referred to as the /septo-temporal/ axis.

*** Hippocampal laminar anatomy: CA1, CA3, entorhinal cortex; their connectivity
The first folding of the sheet described above divides the /proximal-distal axis/ into two parts, named CA3 and CA1 by Lorente de NÃ³ [@lorente1934studies], (CA stands for "cornu ammonis", or ram's horn, which is reminiscent of the shape of the hippocampus in cross section).

CA3 derdrites receive most of their synaptic input from the /dentate gyrus/, entorhinal cortex, and the axons of other CA3 neurons. [TODO]. CA1 receives most of its input from CA3 and entorhinal cortex, but does not project to itself [TODO]. These patterns of inputs are more restricted than in many other parts of the cortex and have lead to computational models that take advantage of a layer with recurrent connections (CA3) connecting to one without (CA1), but none have wide acceptance. We will see later that our understanding of information processing within a single layer is incomplete, and this makes it difficult to speculate on the nature information transmission between areas.

The natural coordinate frame for the hippocampus then are the /proximal-distal/, /septo-temporal/, and /basal-apical/ 

*** Hippocampal place cells
Pyramidal cells increase their firing rate dramatically when rats enter a particular part of a maze, as originally described by O'Keefe [TODO]. The region of space eliciting spikes is that cells's /place field/. Typical place fields are between 20 and 80 centimeters long [TODO], and different neurons have different place fields; recording about 30 neurons is enough to find an 3 meter track without any gaps unrepresented by at least one place field. Spiking rates outside of a neuron's place field are quite low - often less than 0.1 Hz, and in-field rates peak rates are reliable across trials, typically 10-30 Hz; a degree of activity modulation hard to find outside of the sensory periphery.

The behavior of place fields in response to rotations and distortions of the maze is a matter of human curiosity responsible for the demise of fandastical numbers of laboratory rats. This large body of work can be summarized in terms of map displacement, /rate remapping/ and /global remapping/. /Rate remapping/ refers to a change in place field peak firing rate and /global remapping/ a displacement in place field location not necessarily in agreement with the displacements experienced at the same time by other place cells. The rules governing which sort of remapping will result from which types of maze manipulation are baroque, to the point that the neurons themselves disagree on the rules in particular instances and may fall at the same time in different directions [TODO]. But in genaral, minor changes to the appearance of the maze tend to elicit rate remapping [TODO octagon] while radical ones scramble the locations of place fields and produce global remapping [TODO octagon and teleport].

A simular rule of thumb applies to most maze and cue displacement results: place fields tend to follow what we would expect of a rat's top-level model of where he is. Minor enlargements of the maze produce proportional stretching and displacement of the place fields. A rotation of enough maze cues such that North is falsely recognized as the old West will produce the appropriate rotation of place fields with respect to the polse of the earth (and a lack of displacement with respect to the cues)[TODO].


*** Theta oscillations and multiunit spike phase
Electrodes in the hippocampus pick up a 7-10 Hz, large-amplitude rhythm, in addition to somatic spikes. This is called the /theta rhythm/ [TODO vanderwolf?] This rhythm is present when animals are running or in a stationary, attentive state, and during REM sleep [TODO]. Theta oscillations can be found throughout CA1 and CA1, as well as in the dentate gyrus and entorhinal contex, and a host of other cortical and subcortical areas. Collectively the areas expressing theta are known as the /Papez circuit/ [TODO]. Incidentally a lesion to any component of the Papez circuit produces in humans strong anterograde amnesia (as reported in the hippocampal patient H.M. [TODO Squire], thought in fact HM's entorhinal cortex was far more damaged than his hippocampus [TODO Italian slice guy]).

The mechanisms of theta's expression is being explored on two levels: the level of the generation of rhythms in the neurons, and the level of the translation of neural rhythms to extracellular currents [TODO Buzsaki 2002 review]. Neither level is completely understood, despite a large number of studies lesioning or pharmacologically silencing specifically excitatory or inhibitory neurons in various brain regions. 

What is known is that two sources of theta can be pharmacologically distinguished by atropine and NMDA antagonists [TODO Buzsaki atropine, Vertes?], and these two components are associated with different intrinsic frequencies and different behavioral states. /Type 1/ theta is sensitive to atropine delivered systemically [TODO Buzsaki atropine] or directly to the medial septum [TODO]; its intrinsic frequency is about 10 Hz and it is natuarlly elicited by running. /Type 2/ theta is sensitive to disruption of glutamate signaling and is naturally elicited by stationary attention [TODO - and fact-check]. The importance of the septum in theta generation is strongly suggested by the fact that lesions to it nearly eliminate the appearance of theta in the local field potential throughout the rest of the brain. But this is not the whole story, as a dissected hippocampus in a dish will spontaneously express theta after application of acetycholine [TODO]. Interestingly, if that same hippocampus is pharmacologically divided in two along its long axis by application of the GABA agonist muscimol, then the two halves will oscillate at different frequencies, the septal end closer to 10 Hz and the temporal end closer to 6 Hz [TODO - and fact-check].

Quite a few facts are also known about the connection between the theta-rhythmic excitation of neurons and neuropil, and the appearance of theta to an electrode in the form of a local field potential. These details are important and interesting because of a connection between the phase of the oscillation and the activities of place cells (which we will discuss soon), and the phase of such an oscillation is such a finicky thing. For one, applying different filters to the recorded signal is enough to significantly advance or delay theta's apparent phase [TODO digital signal processing]. For another, an electrode's view of the rhythm is determined by the dipole environment local to it, and different parts of a neuron's dendritic tree express different dipoles at different phases of theta; so that the perceived phase of theta changes by half a cycle as an electrode is moved through the thickness of the hippocampus.

Buzsaki [TODO fix name accents] in particular has done a lot of mapping of the electrical sources of theta, first by lowering a single electrode in consistent intervals [TODO], and later by using probes with large numbers of evenly spaced contacts and applying the current-source-density technique [TODO], which predicts the spatial sources of current from structured voltage measurements. To summarize his findings and related ones from other labs, the /Type 1/ theta currents come mostly from inhibitory conductances near the soma [TODO fact-check], and /Type 2/ currents are mainly due to excitatory input to the apical dendrites. These two sources do not agree in phase, and the effects of each drop off with distance of the recording electrode from the source. This is why electrodes with different placement will report different theta phases - they are respectively closer to different theta sources. The combined effects of the multiple sources is still fairly sinusoidal, as the sources are fairly sinusoidal and equal in frequency, and sine waves of equal frequency but different phase generally sum to a sine wave of a new phase [TODO waves book].



*** Theta phase precession & theta sequences

** Sleep states, cortical rhythms, hippocampal-cortical interactions

*** Sleep stages, cortical EEG correlates (spindles, delta, theta)
*** Up-down states in vitro, frames of cortical spikes during sleep in vivo
*** Hippocampal ripples and sleep replay, wake replay
*** Hippocampal-cortical coordination of ripples/spindles
*** Spatial content in CA1 and visual cortex


** Haskell

*** What is functional programming
Functional programming is both a style of programming and a set of language features designed to make functional programs natural to write and performant. That style revolves around two novel notions of what a function is. First, functions in a functional programming language are analogous to functions in math - relationships between inputs in a domain and return values in a range; they are guaranteed to return the same result from the same inputs. Second, functions are themselves 'normal values' - they can be passed as arguments to other functions, or returned from other functions as return values.

Languages like c allow a programmer to use functions in this way but do not make it easy. C is modeled closely on computer hardware, a context that emphasizes allocating memory and manipulating it. These operations are not 'functional' in the mathematical sense, because they involve 'doing' things - fetching memory blocks, performing some activity that is dependent on what was found in the memory block, and modifying the memory block. Functions in math are relationships between values in a domain and a range; these relationships are not dependent on the state of things like memory blocks, and the evaluation of a function's result in math does not impact the world in a way that changes other mathematical equations.

More natural support for functional programming is available in many higher-level languages, for instance python has the builtin functions $map$, which takes a function and a list and returns a list with the function applied to each element. We can write a function that modifies a single number and apply that function to a list of numbers using $map$.

\singlespacing

#+NAME: pythonMap
#+BEGIN_SRC python :results output
  def topLimit(x):
      if x > 10:
          return 10
      else:
          return x

  print map(topLimit,[1,15,20,3,-2,5])
#+END_SRC
#+RESULTS: pythonMap
: [1, 10, 10, 3, -2, 5]

\doublespacing

The $map$ function in Haskell looks very similar, except that there are no parentheses used in applying a function to its arguments. The first line defines the function $topLimit$ as a mapping from number to number, and the second line uses $map$ to apply $topLimit$ to a list of numbers.

\singlespacing

#+NAME: haskellMap
#+BEGIN_SRC haskell :results function
  let topLimit x = if (x > 10) then 10 else x
  print $ map topLimit [1,15,20,3,-2,2]
#+END_SRC
#+RESULTS: haskellMap
| 1 | 10 | 10 | 3 | -2 | 2 |

\doublespacing

*** What are types
Types are sets like $Integer$ or $String$ whose elements are values, like $\{0, 1, -1, 2, -2, ...\}$ and $\{'Greg', 'Hello\ neuron\backslash n', ...\}$ respectively [TODO]. Their role is to annotate data in a program, which would otherwise exist only as $0$ s and $1$ s whose identity would need to be tracked by the programmer. These annotations ensure that functions and data are paired in the correct way - for example preventing the programmer from attemping to take the square root of a string.

That basic motivation for types has been taken much further in the design of different programming languages. The nature of types is the main feature distinguishing programming languages [TODO]. Type systems divide languages into classes like dynamically typed languages (e.g. python, javascript, lisp), in which values can adopt a new type if the context demands it; and statically typed languages (e.g. c++, Java, Haskell), in which they can't. The term 'object oriented programming' refers to one style of type system, in which smaller types can be collected into a larger type called a 'class', and classes can be derived from a parent class [TODO]. The typical example is a $Car$ class that has associated data, such as a $String$ to identify its owner, a pair of $Number$ s to indicate its location, and a $Number$ to indicate its speed. Another class $Truck$ could be derived from $Car$, and the $Truck$ type would inherit the $Car$ s associated data. We can add additional associated data, like a $Number$ type to indicate the maximum payload and a $Number$ to store the tow rating of its trailer hitch. Individual $Car$ s would be constructed in the program with concrete values in all the associated data fields. The goal in an object-oriented design is to build a heirarchy of sets (types, classes) that reflects the heirarchy of labels of objects. Internal properties of the objects being modeled are 'inside' the types, and running a program involves modifying these interval values. Consequently, the style is very noun-oriented [TODO - Yegee post].

An alternative foundation is to model types around logic, capturing ideas like mutual exclusion, associated data, and value-to-value relationships in the types. We need an example here:

\singlespacing

#+BEGIN_SRC haskell
  data Coord = C Double Double  -- (1)

  ptA = C 0.1 0.1 :: Coord      -- (2)
  ptB = C 1.1 0.1 :: Coord
  ptC = C 0.5 2.1 :: Coord

  data SpikeRegion =            -- (3)
      Box       { corners :: (Coord,Coord), bChans :: (Int,Int)}
    | Polygon   { polyPoints :: [Coord],    pChans :: (Int,Int)}      
    | Union     SpikeRegion SpikeRegion
    | Intersect SpikeRegion SpikeRegion
    | Diff      { rBase :: SpikeRegion, rDelete :: SpikeRegion}

regionA :: SpikeRegion          -- (4)
regionA = Box {corners = (ptA, ptB), bChans = (1,2)}

regionB :: SpikeRegion
regionB = Polygon { polyPoints = [ptA,ptB,ptC], pChans = (2,3)}

regionC :: SpikeRegion
regionC = Intersect regionA regionB
#+END_SRC

 - $(1)$: We first define our own set $Coord$, the set of all pairs of $Double$ s (real numbers). The $C$ is a 'constructor' that can be used to build a $Coord$.

 - $(2)$: $ptA$, $ptB$ and $ptC$ are each particular $Coord$ s, built from $C$ and a pair of real numbers.

 - $(3)$: We define a more complicated type, $SpikeRegion$, the set of amplitude regions that could be used to spike-sort tetrode data. A spike region could take one of five forms. The definition of each form is separated by a $|$ and a new line. The $Box$ constructor builds a Spike Region from a pair of $Coord$ s and the pair of electrode channels used for sorting. The terms 'corners' and 'bChans' here are not important - they are just labels for accessing the $Box$ s internal data later. Alternatively, the $Polygon$ constructor can be appleid to a list of $Coord$ s. $Union$ is different; it is built from its constructor and a pair of other $SpikeRegion$ s. Its meaning in our program is: 'the space that is in either of the associated bounding regions'.

 - $(4)$: We define three different regions. The first is a rectangular region defined for tetrode channels 1 and 2. The second is a polygonal region defined by our three $Coord$ s on channels 2 and 3. The third is the intersection of the first two regions. $regionC$ is a typical sort of region used in manual cluster-cutting: the intersection of regions drawn on two projections. A region that uses a third projection to further restrict $regionC$ could be constructed simply as $Intersect\ regionC\ anotherRegion$.

\doublespacing

To declare five mutually-exclusive sorts of regions in Python, we have two options, neithof of which are as intuitive as the Haskell type above. 

First, we could write one class with an associated string that we set to 'box', 'polygon', etc, as well as the sum of all the associated data for all of the possible sorts of regions. This solution allows a programmer using a $SpikeRegion$ type to accidentally use value that does not belong with that sort of region. If we try to refer to one of the $Intersection$ 's sub-regions when our region is a $Box$, our program will crash at some time in execution. An more serious issue would arise if data were used in a way that disagrees with the meaning of the type but does not cause a crash. It would be a very innocent mistake for a programmer to accidentally make use of the $bChans$ data when working with a $Union$ region, believing that they are taking the union of two projections instead of a union within the full 4 channels of a tetrode. This is a silent bug; the program will run but produce incorrect results. In the best case, a user will notice this and the bug will be fixed; in the worst case the error will propagate into the experimental conclusions.

Alternatively, we could use object-oriented style in Python to enfoce the invariant that $Box$ and $Union$ and the others are associated with different sorts of data. The approach would be to define a $GenericRegion$ class with no assaciated data, and one associated 'stub' function for checking whether the generic region contains a spike (the stub will not be implemented - it's a placeholder). Then five new classes can be written for the five types of region. The derived $Box$ class will have fields for the corners of the box and for the channels of the electrode. The derived $Intersection$ class will have two references to other $SpikeRegion$ s. This solution enforces our invariant nicely, but it forces the functions that use a subtype of $SpikeRegion$ to resolve the actual type; and it cost us a lot of boiler-plate code defining all of our subtypes. Aditionally, we have no way to keep track of whether additional classes will be derived in distant files that are part of the same program.

The mechanism of defining data types in Haskell allows (in fact, forces) the programmer to enumerate the variants of a type in one place, circumventing the issues discussed in the context of Python's types. Additionally, because the definition of our data is collected into one place, the compiler can know enough about our type check its use during compilation, before the program is ever run. It would be impossible for the programmer to accidentally refer to the sub-region of a $Polygon$ and produce an error in running code, for example, because the compiler would recognize this as a contradiction in terms ($Polygon$ has no associated sub-region data) and refuse to produce an program from the faulty code. The compiler can also ensure than any function operating on $SpikeRegions$ has handled every possible case of $SpikeRegion$. This checking is a tremendous source of help to a programmer experimenting with changes in the data model. Without this checking, bringing the rest of a program into alignment with a change to a data definition is often done by running the program to the point of failure, finding and fixing one bug at a time.

Functions in Haskell are values and therefore have types. Their types indicate their domain and range. $length$ is a function from $[a]$ (list of any type) to $Integer$. We will also define a tetrode spike and a function for judging whether it is in a region.

\singlespacing

#+BEGIN_SRC haskell
length :: [a] -> Int
length =  undefined   -- we'll implement length soon

data TetrodeSpike = TS [Double]

regionContainsSpike :: SpikeRegion -> TetrodeSpike -> Bool
regionContainsSpike =  undefined
#+END_SRC

\doublespacing

The type of $regionContainsSpike$ looks strange to normal programmers because there is not a clear distinction between arguments and return values. However there is something interesting happening. The $\rightarrow$ in a type is right associative, so $a \rightarrow b \rightarrow c$ is synonymous with $a \rightarrow (b \rightarrow c)$. $regionContainsSpike$ is in fact a function that takes a $SpikeRegion$ and returns a $(TetrodeSpike \rightarrow Bool)$, a function. We can apply this new function to a $TetrodeSpike$ to get a $Bool$. Surprisingly, all functions in Haskell are 'actually' functions of one argument. Multiple-function arguments can always be simulated in terms of single-argument functions that return new functions taking one argument fewer.

*** Declarative programming

Another side of the story of how Haskell facilitates writing code with fewer bugs is /immutability/ - the notion that variables are fixed at a single value throughout a program. The rationalle is that the behavior of a program is much harder to reason about when variables are allowed to change value. All modern languages have facilities for limiting the access of particular variables to specific other regions in the source code, to make this reasoning easier. Haskell goes to the extreme by forbiding any variable from changing.

Removing the ability to change a variable is obviously an enormous restriction on the flexability and usefullness of a language, and it's not immediately clear how many types of programs we could recover in this regime. In fact, this aspect of Haskell was very unflattering during its early history [TODO - lazy with class]. But a great deal of research and practice have resulted in new programming tools, styles and idioms that bridge the gap. After importing something called a /monad/ from abstract mathematics, the notion of change could be integrated into the type system in a highly principled way [TODO Wadler], and now Haskell is an exceptionally good language for coordinating programs with moving parts and uncertainty from the data in the world.

But before resorting to monads, it is usefull to see how many values can be computed without making use of changing variables, using pure mathematical equations instead.

\singlespacing

#+BEGIN_SRC haskell
length :: [a]    -> Int
length    []     =  0
length    (x:xs) =  1 + length xs
#+END_SRC

\doublespacing

In this listing, the function $length$ is defined by parts. The length of the empty list ( [] ) is $0$. The length of any list that can be broken into its first element and a remainder is $1$ more than the length of the remainder. Evaluating the right-hand-side in the non-empty-list case involves a recursive call to length on $xs$. The term $(x:xs)$ on the left-hand side is a way of naming different parts of the input value passed to the function that makes this kind of recursive definition (the definition of functions in general) convenient. These names only apply within the body of the function, they aren't permanently stored or passed into subfunctions. So when we recursively descend into length, the name $xs$ is a different variable in each context, respectively being bound to a smaller sublist. This is easier to see with the names removed completely:

\singlespacing

#+BEGIN_SRC haskell
  length "Rat"           -- matches (R:"at")
= 1 + length "at"        -- matches (a:"t")
= 1 + 1 + length "t"     -- matches (t: [])
= 1 + 1 + 1 + length []  -- matches []
= 1 + 1 + 1 + 0
= 3
#+END_SRC

\doublespacing

Using recursion, we described the length of the list, rather than computing it with iteration, as would be normal in Python for example:

#+BEGIN_SRC python
def listLength(x):
    i = 0
    l = x.copy()
    while
*** Avoiding bugs by lifting program logic into types, compiler catches mistakes early

*** Concurrency - difficulty of running multiple threads simultaneously
*** Software transactional memory


\newpage

* Coordinated information coding in a desynchronized network

** Introduction

*** CA1 place cell excitation is timed by 10Hz oscillation - theta rhythm
*** Tension between hypothesized roles in gating communication channels and encoding
*** Theta as traveling wave, excitatory time offsets over hippocampal CA1
*** Theta-dependent phenomena: locally modulated or globally synchronized?
*** Theta sequences: locally paced or globally synchronized?

** Results

*** Theta phase spatial properties and timing offsets: 20ms delay per mm

-  Theta oscillation recorded from LFP, phase offsets correlated w/
   medial-lateral electrode location
-  Describe traveling-wave model fit to multi-tetrode array phase offset
   pattern
-  Repeat above using multiunit spiking as the measure of theta rhythm,
   comparison to findings with LFP
-  From traveling wave parameters, estimate the timing offset per unit
   anatomical distance in CA1 (20 ms/mm)
 
*** Ensemble theta sequences are synchronized

-  Subdivide tetrodes according to anatomical location, three groups 1mm
   wide
-  Decode position independently in each group at fine timescale
-  Theta sequences in most medial group line up with those in most
   lateral group to better within 10 ms (expected 40 ms if timing
   follows theta verbatim)

*** Place cell pairs synchronize across long distances

-  Considering the distance between two tuning curves (e.g. 0 meters)
   and the anatomical distance between the two place cells (e.g. 1 mm),
   what is the observed timing difference between their spikes?
-  Repeat for all pairs of place cells, measure timing offset as a
   function of place field distance and anatomical distance
-  Measured 15 ms per environmental meter (expected from theta
   sequences)
-  Measure ~3 ms per anatomical mm (lower than predicted from theta time
   offsets)

*** Place field properties correlate weakly with anatomical location

-  Lateral CA1 units tend to be longer
-  No significant correlation between anatomical location and field
   skewness

*** Theta sequences and place cell pairs are synchronized between CA3 and CA1


-  Repeat theta-sequence timing and place-cell pair timing, but
   comparing CA3 to CA1
-  Observe that CA3 and CA1 theta sequences are tightly synchronized,
   despite literature theta timing differences ~25 ms.

** Discussion

*** Theta traveling wave matches previous report: ~20ms/mm delay
*** Despite theta timing differences, information coding is synchronized
*** Different parts of CA1 weakly preferentially carry most of the spike rate at different times 

*** Model 1: Spatially graded, temporally constant compensating excitation

   -  Predictions for place field size match data
   -  Prediction for skewness don't

*** Model 2: Phase precession inherited from synchronized afferents

    - Afferents don't have traveling waves, but CA3 (main input 1)
      is uniformly different phase from entorhinal cortex (main input 2),
      and CA1 phase is inherited from a mixture of these two, according to
      the proportional strength of the inputs at that point. Medial CA1
      gets more heavy EC input and is excited earlier , lateral CA1 more
      heavy CA3 input and is excited later. Phase precessing from
      individual CA1 cells is inherited directly from one input area or the
      other.

   -  Possibly more parsimonious than graded excitation model
   -  Requires CA1 phase to be between CA3 and EC phases, but this isn't
      the case

-  Limitations of this study

   -  Too few units, had to collapse data over time, or average over
      cells
   -  Not sensitive to cycle-by-cycle variations in theta wave
      parameters.

-  New questions

   -  Is theta desynchronized, traveling, within CA3, EC, others?
   -  Which model (1,2, or another) accounts for greater synchrony in
      information content than in underlying excitation?
   -  EC layer 3 grid cells do not phase precess. Do they contribute to
      CA1 timing?
   -  'Medial' and 'lateral' CA1 carry preferentially early and late
      stages of theta sequences, but we only looked at the most medial
      1/3 of CA1 - does this trend continue as you proceed laterally,
      with very lateral place cells prospectively coding far ahead of
      the rat?
   -  'Traveling wave' model often a poor fit to individual cycles. Can
      larger grids of electrodes find a more accurate structure, more
      whole picture
   -  Is the synchrony of place cell coding used downstream? Actively
      maintained in CA1?

\pagebreak

*** Information timing decoupled from bulk firing rate for globally coherent coding
* Real time position decoding from populations of place cells

** Introduction

*** Theta sequences and sequence replay in place cells, phenomenology

   -  Replay occurs during reward consumption & slow-wave sleep
   -  Theta sequences always present during running
   -  Both theta sequences and replay touch parts of the track in a way
      that isn't strictly tied to recency of experience or future goals
   -  However there is a statistical bias during sleep for replay of
      maze experienced just prior to sleep, and statistical bias for
      awake replay to involve salient parts of a maze (start/reward
      location)

*** Summary of semi-indiscriminant replay disruption studies

   -  Disruption of all ripples in sleep slows learning of the more
      recently experienced track.
   -  Disruption of all awake ripples in a working memory task
      interferes with decisions involving working memory, doesn't
      measurably interfere with simpler decisions

*** Rationale for information-dependent replay manipulation

   -  Is replay content in any way under rat's control?

      -  Reward rat for replays that go West, punish for replays that go
         East
      -  Do West-going replays then happen more often?

   -  Are rats aware of their replay content?

      -  Use most recent replay (West or East) to determine which arm of
         a maze will be rewarded
      -  Can rat learn to use their own replay (or its correlates) to
         guide their behavior?

*** Online replay decoding challenges

   -  Tracking rat, isolating units, computing place fields, and
      stimulus decoding all happen offline; need to happen online for
      streaming data
   -  /Throughput requirements:/ must decode at least as quickly as data
      comes in
   -  /Latency requirements:/ data -> decoding lag must be fast enough
      for behavioral feedback, preferably fast enough to disrupt an
      ongoing replay
   -  /Asymptotic requirements:/ Decoding time must not increase with
      duration of experiment, or long experiments ruled out.
   -  /Concurrency:/ Many sources of data (32 tetrodes, tracker, user
      input) all updating a single model

*** Minimizing human intervention: no time for manual spike sorting

*** Choosing the right language for implementation: Haskell

   -  Haskell types model domain very tightly, compiler checks program
      logic
   -  Types let compiler check whole codebase during code rewrites /
      code experiments
   -  Types tell runtime system which operations are pure
      (not-interacting), very nice property for concurrency



** Materials and Methods

*** Backend signal acquisition and networking

-  Compatible with existing recording system, runs side by side with
   shared timestamps
-  NiDAQ cards, 64 channels at 32 kHz
-  Software spike filtering, software LFP filtering
-  Software grouping of channels into tetrodes, spike detection
-  Publish spikes and LFP to the network for other programs to process

*** Offline position reconstruction

-  Manually sort spikes from many cells on single tetrode into groups,
   recover single-cell spike trains
-  Turn rat location on curved track into simple stimulus for prediction

   -  Distance along track
   -  'Outbound' or 'inbound' running direction

-  Compute likelihood of spike rate given stimulus, using data from
   whole session
-  For a given ~15ms time window, use spikes in that time window and
   matching spike rate likelihood functions (place fields) to predict
   stimulus (track pos) by Bayesian inference

*** Online position reconstruction

-  Manual spike sorting probably far too slow, use semi-automated or
   clusterless
-  Choosing data structure for spike sorting & decoding with bounded
   memory & time use
-  Likelihood functions have to be updated during experiment

   -  By a lot of threads (~ 32 tetrodes * spike rate, plus current
      position)
   -  Decoder also writes to likelihood function

-  Use Haskell's concurrency library to coordinate many writing/reading
   threads

** Results

*** Decoding quality: theta sequences and replay

   -  Offline position reconstruction compared to online with clusters,
      online clusterless
   -  Tracking of rat's position
   -  Appearance of theta sequences
   -  Appearance of replay

*** Decoding speed and realtime requeriments
*** Bugs, deadlocks, crashes and refactorings

** Discussion

*** Recap: designed tool for decoding streaming place cell data
*** Remaining components needed to run experiments

   -  Networked rat tracker and track linearizer
   -  Online line-finding algorithm
   -  Combining estimates from multiple computers (for > 16 tetrode
      case)
*** Experimental goals with sequence replay
*** Extension to non-hippocampal contexts

\pagebreak

* Retrosplenial slow-wave wake and interaction with hippocampus

** Introduction

*** Cortico-hippocampal sleep interactions, possible role in memory

-  Two phase consolidation model: encode at wake, burn-in during sleep
-  HPC ripples correlated w/ sleep CTX sleep spindles - communication
   signature?
-  Regular interval between hippocampal frame onset and cortical frame
   onset

*** Slow wave oscillations cleanly destinguish between sleeping and awake cortex
*** Ripples cleanly destinguish between 'online' and 'offline' hippocampus
*** Retrosplenial cortex unexpectedly follows HPC into SWS-like state during reward

** Results

*** Characterizing slow-wave sleep (SWS) in cortex

-  Examples of light sleep, spindles, frames and K-complexes in LFP,
   spiking
-  Examples of deep sleep, frames and K-complexes in LFP, spiking
-  Distribution of activity over all cortical electrodes
-  Average up-state length, down-state length

*** Retrosplenial cortex enters SWS-like state during novelty / large rewards

-  Examples
-  Average up-state length, down-state length

*** RSC awake slow waves coordinate with hippocampal ripples

-  5-second window showing co-transition into SWS-like state (RSC
   frames, HPC ripples & replay)
-  200-second window showing behavioral-timescale relationship
-  Cross-correlation of ripples & RSC frames similar between wake and
   SWS

*** RSC awake slow waves require large reward in well-trained rats

-  Occur at most stopping points early in training
-  After ~1 week, spontaneous frames & small-reward frames stop, but
   large-reward frames persist (for at least a month)

*** Anatomical restriction - nonparticipation in other cortical areas

-  Simultaneously recorded somatosensory, motor, posterior parietal
   cortex have no frame-like activity (noticeable changes in spike rate
   or LFP) during RSC awake frames

*** Slow-wave wake not limited to times of sleepiness

-  Awake SWS-like activity continues in both light and dark phases of
   light cycle
-  Many SW's are flanked by fast running and chewing

** Discussion

*** Recap: Awake slow-waves in RSC, coordinated with HPC, fully awake
*** In HPC-Cortex interaction, Online/offline vs. awake/asleep
*** Functional roles for HPC-Cortex coordination may apply to wake
*** New questions raised by SWW: mechanism and function

-  New questions:

   -  What other brain areas have SWWake? Papez circuit?
   -  What's the mechanism for the switch from awake-aroused to SWW
      cortex?
   -  What causes Slow Waves to traverse all of cortex during sleep, and
      not wake?
   -  Is there information content in slow-wave frame spikes? Is it
      bounded by slow wave boundaries in an interesting way?

\pagebreak

** Materials & Methods

-  10 tetrodes in HPC, 10 tetrodes split between retrosplenial,
   somatosensory, motor, posterior parietal cortex
-  Trained rats to run circular track for reward every 270 degrees CCW

\newpage

* Conclusion / Wrap-up

Brief summary of the role of populations of neurons in hippocampal
spatial coding. Much more reliability in the timing of place cell spike
sequences than there is in single cell measures like phase precession.
We want to know if population sequences are an essential feature of
coding, or just a means of denoising, and answering that question will
involve manipulations that account for information content in and react
to it in real time, as well as studies of how population sequences are
interpreted by downstream cortical areas.

\newpage

* References
