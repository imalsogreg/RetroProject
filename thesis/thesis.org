
* Overview

Three chapters about populations of neurons in the hippocampus coordinating their activity on fine timescales, and how that coordination relates to the larger context of hippocampal inputs, outputs, and experimental measurements. In the first chapter, explore the lack of synchrony in timing of the brain rhythms underlying excitatory drive in hippocampal place cells, and the impact of those differences on the ability of distant place cells to coordinate reliably during population-wide coding events. In the second chapter, describe the development of a tool for detecting place cell population coding events and using the information they contain to manipulate brain activity or behavior experimentally. In the third chapter, describe the unexpected transition of retrosplenial cortex into a slow-wave-sleep like state during reward consumption, and the timing relationship between hippocampal activity and retrosplenial cortex during these events. Conclude with a discussion about coordinated encoding that ties the three chapters together - how does the sequential nature of coding in hippocampus relate to coding in the afferents and efferents - do they also encode information in spike sequences? What experiments using real-time ensemble decoding would help answer those questions?



* General Background

** Hippocampal anatomy and physiology relevant to information coding

A brief reveiw of cellular organization in the hippocampus is given to help the reader stay oriented during discussions of electrode placement and traveling wave propagation. We also describe the freely moving rat's local field potential signatures and single-unit spiking properties, which are central to the rest of the thesis.

*** Hippocampal anatomy: cell layer and dendritic layers
The rat hippocampus is a curved, complex, three-dimensional structure most easily thought of as a sheet, about 10 by 7 mm face-on and 1mm thick, folded into a 'C' shape first along its short axis, and again into a larger 'C' along its long axis. The face of the sheet is fully tiled by primarily excitatory pyramidal neurons. Their cell bodies of these neurons are collected into a thin (about 0.1mm) band whithin the sheet's 1mm thickness. 

Basal dendritic arbors extending upward from the cell bodies toward the skull (the hippocampus is inverted relative to cortex) for 0.25mm form the /stratum oriens/. Apical dendrites extend downward for 0.5mm forming the /stratum radiatum/, and then branch widely to form the /stratum laconosum moleculare/.

After folding and curling, the far end along the longer dimension of the sheet terminates near the septal nuclei, and the other travels backward and bends down to embed itself in temporal cortex. The long axis of the hippocampus is referred to as the /septo-temporal/ axis.

*** Hippocampal laminar anatomy: CA1, CA3, entorhinal cortex; their connectivity
The first folding of the sheet described above divides the /proximal-distal axis/ into two parts, named CA3 and CA1 by Lorente de NÃ³ [@lorente1934studies], (CA stands for "cornu ammonis", or ram's horn, which is reminiscent of the shape of the hippocampus in cross section).

CA3 derdrites receive most of their synaptic input from the /dentate gyrus/, entorhinal cortex, and the axons of other CA3 neurons. [TODO]. CA1 receives most of its input from CA3 and entorhinal cortex, but does not project to itself [TODO]. These patterns of inputs are more restricted than in many other parts of the cortex and have lead to computational models that take advantage of a layer with recurrent connections (CA3) connecting to one without (CA1), but none have wide acceptance. We will see later that our understanding of information processing within a single layer is incomplete, and this makes it difficult to speculate on the nature information transmission between areas.

The natural coordinate frame for the hippocampus then are the /proximal-distal/, /septo-temporal/, and /basal-apical/ 

*** Hippocampal place cells
Pyramidal cells increase their firing rate dramatically when rats enter a particular part of a maze, as originally described by O'Keefe [TODO]. The region of space eliciting spikes is that cells's /place field/. Typical place fields are between 20 and 80 centimeters long [TODO], and different neurons have different place fields; recording about 30 neurons is enough to find an 3 meter track without any gaps unrepresented by at least one place field. Spiking rates outside of a neuron's place field are quite low - often less than 0.1 Hz, and in-field rates peak rates are reliable across trials, typically 10-30 Hz; a degree of activity modulation hard to find outside of the sensory periphery.

The behavior of place fields in response to rotations and distortions of the maze is a matter of human curiosity responsible for the demise of fandastical numbers of laboratory rats. This large body of work can be summarized in terms of map displacement, /rate remapping/ and /global remapping/. /Rate remapping/ refers to a change in place field peak firing rate and /global remapping/ a displacement in place field location not necessarily in agreement with the displacements experienced at the same time by other place cells. The rules governing which sort of remapping will result from which types of maze manipulation are baroque, to the point that the neurons themselves disagree on the rules in particular instances and may fall at the same time in different directions [TODO]. But in genaral, minor changes to the appearance of the maze tend to elicit rate remapping [TODO octagon] while radical ones scramble the locations of place fields and produce global remapping [TODO octagon and teleport].

A simular rule of thumb applies to most maze and cue displacement results: place fields tend to follow what we would expect of a rat's top-level model of where he is. Minor enlargements of the maze produce proportional stretching and displacement of the place fields. A rotation of enough maze cues such that North is falsely recognized as the old West will produce the appropriate rotation of place fields with respect to the polse of the earth (and a lack of displacement with respect to the cues)[TODO].


*** Theta oscillations and multiunit spike phase
Electrodes in the hippocampus pick up a 7-10 Hz, large-amplitude rhythm, in addition to somatic spikes. This is called the /theta rhythm/ [@vanderwolf1969hippocampal]. The theta rhythm is present when animals are running or in a stationary, attentive state, and during REM sleep [TODO]. Theta oscillations can be found throughout CA1 and CA1, as well as in the dentate gyrus and entorhinal contex, and a host of other cortical and subcortical areas. Collectively the areas expressing theta are known as the /Papez circuit/ [TODO]. Incidentally a lesion to any component of the Papez circuit produces in humans strong anterograde amnesia (as reported in the hippocampal patient H.M. [TODO Squire], thought in fact HM's entorhinal cortex was far more damaged than his hippocampus [TODO Italian slice guy]).

The mechanisms of theta's expression is being explored on two levels: the level of the generation of rhythms in the neurons, and the level of the translation of neural rhythms to extracellular currents [TODO Buzsaki 2002 review]. Neither level is completely understood, despite a large number of studies lesioning or pharmacologically silencing specifically excitatory or inhibitory neurons in various brain regions. 

What is known is that two sources of theta can be pharmacologically distinguished by atropine and NMDA antagonists [TODO Buzsaki atropine, Vertes?; @kocsis1999interdependence], and these two components are associated with different intrinsic frequencies and different behavioral states. /Type 1/ theta is sensitive to atropine delivered systemically [TODO Buzsaki atropine] or directly to the medial septum [TODO]; its intrinsic frequency is about 10 Hz and it is natuarlly elicited by running. /Type 2/ theta is sensitive to disruption of glutamate signaling and is naturally elicited by stationary attention [TODO - and fact-check]. The importance of the septum in theta generation is strongly suggested by the fact that lesions to it nearly eliminate the appearance of theta in the local field potential throughout the rest of the brain. But this is not the whole story, as a dissected hippocampus in a dish will spontaneously express theta after application of acetycholine [TODO]. Interestingly, if that same hippocampus is pharmacologically divided in two along its long axis by application of the GABA agonist muscimol, then the two halves will oscillate at different frequencies, the septal end closer to 10 Hz and the temporal end closer to 6 Hz [TODO - and fact-check].

Quite a few facts are also known about the connection between the theta-rhythmic excitation of neurons and neuropil, and the appearance of theta to an electrode in the form of a local field potential. These details are important and interesting because of a connection between the phase of the oscillation and the activities of place cells (which we will discuss soon), and the phase of such an oscillation is such a finicky thing. For one, applying different filters to the recorded signal is enough to significantly advance or delay theta's apparent phase [TODO digital signal processing]. For another, an electrode's view of the rhythm is determined by the dipole environment local to it, and different parts of a neuron's dendritic tree express different dipoles at different phases of theta; so that the perceived phase of theta changes by half a cycle as an electrode is moved through the thickness of the hippocampus.

Buzsaki [TODO fix name accents] in particular has done a lot of mapping of the electrical sources of theta, first by lowering a single electrode in consistent intervals [TODO], and later by using probes with large numbers of evenly spaced contacts and applying the current-source-density technique [TODO], which predicts the spatial sources of current from structured voltage measurements. To summarize his findings and related ones from other labs, the /Type 1/ theta currents come mostly from inhibitory conductances near the soma [TODO fact-check], and /Type 2/ currents are mainly due to excitatory input to the apical dendrites. These two sources do not agree in phase, and the effects of each drop off with distance of the recording electrode from the source. This is why electrodes with different placement will report different theta phases - they are respectively closer to different theta sources. The combined effects of the multiple sources is still fairly sinusoidal, as the sources are fairly sinusoidal and equal in frequency, and sine waves of equal frequency but different phase generally sum to a sine wave of a new phase [TODO waves book].




** Sleep states, cortical rhythms, hippocampal-cortical interactions

*** Sleep stages, cortical EEG correlates (spindles, delta, theta)
Waking and sleep behavioral states are accompanied by very different patterns of neural activity in the cortex and hippocampus. These differences can be more pronounced that the modulations of firing rate associated with encoding sensory information and producing particular motor output. Some are easily detected through the human skull via EEG. They are immediately apparent in the firing patterns of populations of neurons and in the character of local field potentials recorded inside the brain. We study these differences in order to understand the function and mechanistic origins of sleep, as well as to gain clues about how sensory stimuli are processed during waking and shut out during sleep. We would also like to know whether the particular signature activity patterns of sleep have functional roles themselves, or in combination with other sleep-specific events in other brain areas.

Sleep in most animals [@lesku2011ostriches] is divided into two categories [@rechtschaffen1968manual]: slow-wave sleep, named after the EEG oscillation associated with it [@steriade1993thalamocortical]; and REM sleep, named for the associated rapid movement of the eyes [@aserinsky1953regularly]. The latter is also called ``paradoxical sleep"[fn:: Perhaps if most sleep were REM sleep, then our nomenclature would be different, and what is now known as slow-wave sleep would have been called ``paradoxical activity", because sleep itself would no longer be the major discriminator between types of brain activity.], because brain activity during REM sleep so closely resembles brain activity in waking animals and because a relative small fraction of mamallian sleep in REM sleep [@roffwarg1966ontogenetic]. 

Slow-wave sleep in humans can be further divided into four stages that correspond to four levels of depth of sleep and are marked by different combinations of signature EEG events[@rechtschaffen1968manual]. Stage I sleep is drowsiness and it is marked by the appearance of spindles, short bouts of ~10Hz oscillations lasting about a second each and occurring from once a minute to ten times a minute until drowsiness transitions into the next sleep stage. In stage II spindles become less freqent and are replaced by occassional K-complexes, large, one-cycle oscillations. Stage III and Stage IV sleep are both deep sleep; they lack spindles altogether, and K-complexes have become part of a 1-4 Hz ``delta oscillation" [@de2000spontaneous], the characteristic rhythm that gives slow-wave sleep its name. Stages III and IV are differentiated by the regularity of the slow waves (they are not very periodic, by the standards of most waves) - in Stage III single cycles will vary in length from 200 ms to 2 s.


*** Up-down states in vitro, frames of cortical spikes during sleep in vivo
Delta oscillations of slow-wave sleep have a clear neural basis, the synchronized spontaneous oscillations of cortical neurons [@steriade2003neuronal]. The extracellural peaks of the slow oscillation correspond to periods of profound hyperpolarization and the complete absence of cortical spiking. Intracellular voltage traces make the non-periodic nature of the slow oscillation more clear; rather than a harmonic oscillation, there is a clear bimodality in the distribution of membrane voltages: ~100ms blocks of -80mV without spiking interspersed between blocks of -60mV with spiking. There is a large body of work on the mechanisms of generation of this state; it is thought to involve interaction with the thalamus (and in particular, inhibitory long-range neurons of the thalamic reticular nucleus), which also profoundly changes its mode of firing during sleep [@marks1993spontaneous].

*** Hippocampal ripples and sleep replay, wake replay
Activity in the hippocampus during awake active behavior is dominated by the 10 Hz theta oscillation, wich impacts the spiking of all neurons in the hippocampus and is plainly visible in local field potential recording and audible in the multi-unit spiking [@grastyan1959hippocampal; @vanderwolf1969hippocampal]. The hippocampus of a rat in slow-wave sleep is very different. It is known as ``large irregular activity"[@ylinen1995sharp], and it consists of quiet periods interrupted by sporadic, vigorous bursts of spikes. The collective activity of the neurons causes a ~200 Hz oscillation known as a ``ripple", which is usually accompanied by a single cycle of a slower (~10 Hz) ``sharp wave". These bursts of activity tend to last about 50 ms [@nguyen2009characterizing], and often come in pairs or tripplets [@layton2013temporal] of ripples in quick succession (a ``ripple burst").

Unlike the cortex, however, sleep is not the primary determinant of which mode of activity the hippocampus will be in. Large irregular activity occurs in the hippocampus whenever an animal stops to groom, consumes reward, or reaches the end of coppulation [@kurtz1973electrophysiological]. Paradoxical sleep in the hippocampus is marked by a sustained theta rhythm and the absense of large irregular activity [@vanderwolf1969hippocampal].

*** Hippocampal-cortical coordination
Several recent studies have begun to show a link between the sleep characteristics of cortex and those of hippocampus. Although the hippocampus does not exhibit delta-frequency slow waves in its local field potential, up-down states mirroring those in cortex are also present in the membrane potential of hippocampal interneurons and the granule cells of the hippocampal input structure, dentate gyrus [@hahn2006phase]. And the membrate potentials of some hippocampal CA3 and CA1 pyramidal cells are modulated at the times of cortical transitions from down state to up state [@hahn2007differential].

The hippocampal ripples have also been found to coordinate in time with cortical down-to-up state transitions [@battaglia2004hippocampal] and cortical spindles [@siapas1998coordinated; @sirota2003communication], suggesting that these oscillatory events may be either a reflection of information transfer between cortex and hippocampus, or that they may be a mechanism by which that information is transferred.

*** Spatial content in CA1 and visual cortex

The possibility of cortico-hippocampal information transfer became much more concrete after the report by Ji and Wilson [ji2006coordinated] that hippocampus and cortex express coordinated information content during slow-wave sleep. Surprisingly, single neurons in primary visual cortex (V1) fire spikes at specific locations on a maze, when that maze is instrumented with visual cues on its floor (1 bit per spike in V1, /vs./ 3 bits per spike in hippocampus); and during sleep, the sequence of V1 cells activated by the track spontaneously co-fire in the same order during slow wave sleep, as place cells have been shown to do in slow-wave sleep [@lee2002memory]. When both hippocampus and cortex reexpress in sleep a sequence learned in wake (admittently a rare event, at least with our current recording capabilities), they almost always agree on /what/ sequence to replay [@ji2006coordinated].

** Haskell

*** What is functional programming
Functional programming is both a style of programming and a set of language features designed to make functional programs natural to write and performant. That style revolves around two novel notions of what a function is. First, functions in a functional programming language are analogous to functions in math - relationships between inputs in a domain and return values in a range; they are guaranteed to return the same result from the same inputs. Second, functions are themselves 'normal values' - they can be passed as arguments to other functions, or returned from other functions as return values.

Languages like c allow a programmer to use functions in this way but do not make it easy. C is modeled closely on computer hardware, a context that emphasizes allocating memory and manipulating it. These operations are not 'functional' in the mathematical sense, because they involve 'doing' things - fetching memory blocks, performing some activity that is dependent on what was found in the memory block, and modifying the memory block. Functions in math are relationships between values in a domain and a range; these relationships are not dependent on the state of things like memory blocks, and the evaluation of a function's result in math does not impact the world in a way that changes other mathematical equations.

More natural support for functional programming is available in many higher-level languages, for instance python has the builtin functions $map$, which takes a function and a list and returns a list with the function applied to each element. We can write a function that modifies a single number and apply that function to a list of numbers using $map$.

\singlespacing

#+NAME: pythonMap
#+BEGIN_SRC python :results output
  def topLimit(x):
      if x > 10:
          return 10
      else:
          return x

  print map(topLimit,[1,15,20,3,-2,5])
#+END_SRC
#+RESULTS: pythonMap
: [1, 10, 10, 3, -2, 5]

\doublespacing

The $map$ function in Haskell looks very similar, except that there are no parentheses used in applying a function to its arguments. The first line defines the function $topLimit$ as a mapping from number to number, and the second line uses $map$ to apply $topLimit$ to a list of numbers.

\singlespacing

#+NAME: haskellMap 
#+BEGIN_SRC haskell :tangle tangles/haskellMap.hs
module Main where

  topLimit x
    | x > 10    = 10 
    | otherwise = x  

  main = print( map topLimit [1,15,20,3,-2,2] )
#+END_SRC

#+BEGIN_SRC sh :results output :exports results
runhaskell tangles/haskellMap.hs
#+END_SRC

#+RESULTS:
[1   | 10 | 10 | 3 | -2 | 2]


\doublespacing

*** What are types
Types are sets like $Integer$ or $String$ whose elements are values, like $\{0, 1, -1, 2, -2, ...\}$ and $\{'Greg', 'Hello\ neuron\backslash n', ...\}$ respectively [TODO]. Their role is to annotate data in a program, which would otherwise exist only as $0$ s and $1$ s whose identity would need to be tracked by the programmer. These annotations ensure that functions and data are paired in the correct way - for example preventing the programmer from attemping to take the square root of a string.

That basic motivation for types has been taken much further in the design of different programming languages. The nature of types is the main feature distinguishing programming languages [TODO]. Type systems divide languages into classes like dynamically typed languages (e.g. python, javascript, lisp), in which values can adopt a new type if the context demands it; and statically typed languages (e.g. c++, Java, Haskell), in which they can't. The term 'object oriented programming' refers to one style of type system, in which smaller types can be collected into a larger type called a 'class', and classes can be derived from a parent class [TODO]. The typical example is a $Car$ class that has associated data, such as a $String$ to identify its owner, a pair of $Number$ s to indicate its location, and a $Number$ to indicate its speed. Another class $Truck$ could be derived from $Car$, and the $Truck$ type would inherit the $Car$ s associated data. We can add additional associated data, like a $Number$ type to indicate the maximum payload and a $Number$ to store the tow rating of its trailer hitch. Individual $Car$ s would be constructed in the program with concrete values in all the associated data fields. The goal in an object-oriented design is to build a heirarchy of sets (types, classes) that reflects the heirarchy of labels of objects. Internal properties of the objects being modeled are 'inside' the types, and running a program involves modifying these interval values. Consequently, the style is very noun-oriented [TODO - Yegee post].

An alternative foundation is to model types around logic, capturing ideas like mutual exclusion, associated data, and value-to-value relationships in the types. We need an example here:

\singlespacing

#+BEGIN_SRC haskell
  data Coord = C Double Double  -- (1)

  ptA = C 0.1 0.1 :: Coord      -- (2)
  ptB = C 1.1 0.1 :: Coord
  ptC = C 0.5 2.1 :: Coord

  data SpikeRegion =            -- (3)
      Box       { corners :: (Coord,Coord), bChans :: (Int,Int)}
    | Polygon   { polyPoints :: [Coord],    pChans :: (Int,Int)}      
    | Union     SpikeRegion SpikeRegion
    | Intersect SpikeRegion SpikeRegion
    | Diff      { rBase :: SpikeRegion, rDelete :: SpikeRegion}

regionA :: SpikeRegion          -- (4)
regionA = Box {corners = (ptA, ptB), bChans = (1,2)}

regionB :: SpikeRegion
regionB = Polygon { polyPoints = [ptA,ptB,ptC], pChans = (2,3)}

regionC :: SpikeRegion
regionC = Intersect regionA regionB
#+END_SRC

 - $(1)$: We first define our own set $Coord$, the set of all pairs of $Double$ s (real numbers). The $C$ is a 'constructor' that can be used to build a $Coord$.

 - $(2)$: $ptA$, $ptB$ and $ptC$ are each particular $Coord$ s, built from $C$ and a pair of real numbers.

 - $(3)$: We define a more complicated type, $SpikeRegion$, the set of amplitude regions that could be used to spike-sort tetrode data. A spike region could take one of five forms. The definition of each form is separated by a $|$ and a new line. The $Box$ constructor builds a Spike Region from a pair of $Coord$ s and the pair of electrode channels used for sorting. The terms 'corners' and 'bChans' here are not important - they are just labels for accessing the $Box$ s internal data later. Alternatively, the $Polygon$ constructor can be appleid to a list of $Coord$ s. $Union$ is different; it is built from its constructor and a pair of other $SpikeRegion$ s. Its meaning in our program is: 'the space that is in either of the associated bounding regions'.

 - $(4)$: We define three different regions. The first is a rectangular region defined for tetrode channels 1 and 2. The second is a polygonal region defined by our three $Coord$ s on channels 2 and 3. The third is the intersection of the first two regions. $regionC$ is a typical sort of region used in manual cluster-cutting: the intersection of regions drawn on two projections. A region that uses a third projection to further restrict $regionC$ could be constructed simply as $Intersect\ regionC\ anotherRegion$.

\doublespacing

To declare five mutually-exclusive sorts of regions in Python, we have two options, neithof of which are as intuitive as the Haskell type above. 

First, we could write one class with an associated string that we set to 'box', 'polygon', etc, as well as the sum of all the associated data for all of the possible sorts of regions. This solution allows a programmer using a $SpikeRegion$ type to accidentally use value that does not belong with that sort of region. If we try to refer to one of the $Intersection$ 's sub-regions when our region is a $Box$, our program will crash at some time in execution. An more serious issue would arise if data were used in a way that disagrees with the meaning of the type but does not cause a crash. It would be a very innocent mistake for a programmer to accidentally make use of the $bChans$ data when working with a $Union$ region, believing that they are taking the union of two projections instead of a union within the full 4 channels of a tetrode. This is a silent bug; the program will run but produce incorrect results. In the best case, a user will notice this and the bug will be fixed; in the worst case the error will propagate into the experimental conclusions.

Alternatively, we could use object-oriented style in Python to enfoce the invariant that $Box$ and $Union$ and the others are associated with different sorts of data. The approach would be to define a $GenericRegion$ class with no assaciated data, and one associated 'stub' function for checking whether the generic region contains a spike (the stub will not be implemented - it's a placeholder). Then five new classes can be written for the five types of region. The derived $Box$ class will have fields for the corners of the box and for the channels of the electrode. The derived $Intersection$ class will have two references to other $SpikeRegion$ s. This solution enforces our invariant nicely, but it forces the functions that use a subtype of $SpikeRegion$ to resolve the actual type; and it cost us a lot of boiler-plate code defining all of our subtypes. Aditionally, we have no way to keep track of whether additional classes will be derived in distant files that are part of the same program.

The mechanism of defining data types in Haskell allows (in fact, forces) the programmer to enumerate the variants of a type in one place, circumventing the issues discussed in the context of Python's types. Additionally, because the definition of our data is collected into one place, the compiler can know enough about our type check its use during compilation, before the program is ever run. It would be impossible for the programmer to accidentally refer to the sub-region of a $Polygon$ and produce an error in running code, for example, because the compiler would recognize this as a contradiction in terms ($Polygon$ has no associated sub-region data) and refuse to produce an program from the faulty code. The compiler can also ensure than any function operating on $SpikeRegions$ has handled every possible case of $SpikeRegion$. This checking is a tremendous source of help to a programmer experimenting with changes in the data model. Without this checking, bringing the rest of a program into alignment with a change to a data definition is often done by running the program to the point of failure, finding and fixing one bug at a time.

Functions in Haskell are values and therefore have types. Their types indicate their domain and range. $length$ is a function from $[a]$ (list of any type) to $Integer$. We will also define a tetrode spike and a function for judging whether it is in a region.

\singlespacing

#+BEGIN_SRC haskell
length :: [a] -> Int
length =  undefined   -- we'll implement length soon

data TetrodeSpike = TS [Double]

regionContainsSpike :: SpikeRegion -> TetrodeSpike -> Bool
regionContainsSpike =  undefined
#+END_SRC

\doublespacing

The type of $regionContainsSpike$ looks strange to normal programmers because there is not a clear distinction between arguments and return values. However there is something interesting happening. The $\rightarrow$ in a type is right associative, so $a \rightarrow b \rightarrow c$ is synonymous with $a \rightarrow (b \rightarrow c)$. $regionContainsSpike$ is in fact a function that takes a $SpikeRegion$ and returns a $(TetrodeSpike \rightarrow Bool)$, a function. We can apply this new function to a $TetrodeSpike$ to get a $Bool$. Surprisingly, all functions in Haskell are 'actually' functions of one argument. Multiple-function arguments can always be simulated in terms of single-argument functions that return new functions taking one argument fewer.

*** Declarative programming

Another side of the story of how Haskell facilitates writing code with fewer bugs is /immutability/ - the notion that variables are fixed at a single value throughout a program. The rationalle is that the behavior of a program is much harder to reason about when variables are allowed to change value. All modern languages have facilities for limiting the access of particular variables to specific other regions in the source code, to make this reasoning easier. Haskell goes to the extreme by forbiding any variable from changing.

Removing the ability to change a variable is obviously an enormous restriction on the flexability and usefullness of a language, and it's not immediately clear how many types of programs we could recover in this regime. In fact, this aspect of Haskell was very unflattering during its early history [TODO - lazy with class]. But a great deal of research and practice have resulted in new programming tools, styles and idioms that bridge the gap. After importing something called a /monad/ from abstract mathematics, the notion of change could be integrated into the type system in a highly principled way [TODO Wadler], and now Haskell is an exceptionally good language for coordinating programs with moving parts and uncertainty from the data in the world.

But before resorting to monads, it is usefull to see how many values can be computed without making use of changing variables, using pure mathematical equations instead.

\singlespacing

#+BEGIN_SRC haskell
length :: [a]    -> Int
length    []     =  0
length    (x:xs) =  1 + length xs
#+END_SRC

\doublespacing

In this listing, the function $length$ is defined by parts. The length of the empty list ( [] ) is $0$. The length of any list that can be broken into its first element and a remainder is $1$ more than the length of the remainder. Evaluating the right-hand-side in the non-empty-list case involves a recursive call to length on $xs$. The term $(x:xs)$ on the left-hand side is a way of naming different parts of the input value passed to the function that makes this kind of recursive definition (the definition of functions in general) convenient. These names only apply within the body of the function, they aren't permanently stored or passed into subfunctions. So when we recursively descend into length, the name $xs$ is a different variable in each context, respectively being bound to a smaller sublist. This is easier to see with the names removed completely:

\singlespacing

#+BEGIN_SRC haskell
  length "Rat"           -- matches (R:"at")
= 1 + length "at"        -- matches (a:"t")
= 1 + 1 + length "t"     -- matches (t: [])
= 1 + 1 + 1 + length []  -- matches []
= 1 + 1 + 1 + 0
= 3
#+END_SRC

\doublespacing

Using recursion, we described the length of the list, rather than computing it with iteration as we would in Python:

\singlespacing

#+BEGIN_SRC python :results output :exports both
def listLength(x):
    nElem = 0
    for i in x:
        nElem = nElem + 1
    return nElem 

print listLength('Hello iteration.')
#+END_SRC

#+RESULTS:
: 16

\doublespacing

The differences between declarative and traditional styles becomes more clear when we combine pieces together into larger programs. Let's try to take the product of the first 10 elemnts of the Fibbonacci sequence.

\singlespacing

#+BEGIN_SRC python :results output :exports both
def listProduct(xs):
    acc = 1
    for n in xs:
        acc = acc * n;
    return acc

def makeFibbonacci(nElems):
    fibs = [1,1]
    for n in range(2,nElems):
        fibs.insert(n,fibs[n-1] + fibs[n-2])
    return fibs

print listProduct(makeFibbonacci(10))

#+END_SRC

#+RESULTS:
: 122522400

\doublespacing

This works, but it's a little unsatisfying to have to say how to build an array filled with Fibbonacci numbers, instead of describing the series itself, and that the definition is tangled up with an arbitrary detail, the length of the list we want to produce. What would happen if we needed the $1000000000$th number, and the array didn't fit it memory? Should we have used something more complicated like a generator, to produce Fibbonacci numbers without using increasing amounts of memory? This would force any users of $makeFibbonacci$ to consume streams. In Haskell, we use recursion to declare what Fibbonacci numbers are:

\singlespacing

#+BEGIN_SRC haskell :tangle tangles/fib.hs
fibs :: [Integer]
fibs = [1,1] ++ (zipWith (+) fibs (tail fibs))
#+END_SRC

\doublespacing

Translating this into English, $fibs$ is the list [1,1] followed by the list-wise sum of $fibs$ with $fibs$ less its first element. The surprising fact that we can define a value in terms of itself comes from the fact that, in math, we don't need to know the entire list $fibs$ in order to apply a function to $fibs$, we only need to know enough about $fibs$ to satisfy what will be used by the function. Here $fibs$ is defined as a seed and a function that only needs the seed in order to produce the next value. But this is an implementation detail. From the point of view of the programmer, we have our hands on a value $fibs$ that is indistinguiquasble from the entire infinite series. Trying to take the product of the list will take infinite time, not because our definition is infinite, but because we are doing something infinite. On the other hand, we can do something finite with something infinite, and it will take only finite time.

\singlespacing

#+BEGIN_SRC haskell :tangle tangles/fibprod.hs
module Main where

fibs = [1,1] ++ (zipWith (+) fibs (tail fibs))
prodFibs n = product (take n fibs)
main = print(prodFibs 10)
#+END_SRC

#+BEGIN_SRC sh :results output :exports results
runhaskell tangles/fibprod.hs
#+END_SRC

#+RESULTS:
: 122522400

\doublespacing

This property of being able to apply functions to arguments when the arguments aren't fully known is called $laziness$, and it is one of the main features allowing declarative programming style and the combination of diverse software components into large programs. Combining infinite things with other infinite things in finite time, and decoupling mathematical models from details about how many elements to generate, are central to that. For an excellent discussion of functional programming's role as a glue layer between large components see Why Functional Programming Matters [@hughes1989functional].


*** Concurrency - difficulty of running multiple threads simultaneously
Concurrency is the simultaneous executition of multiple sections of code simultaneously [fn:: or interleaved in time with one another quickly enough that they appear to be executing simultaneously]. The Haskell standard library provides the function $forkIO$, which takes as its input a single action to run and acquires a new thread from the Haskell runtime to run that action.

\singlespacing

#+NAME: forkIO
#+BEGIN_SRC haskell :tangle tangle/forkIOExample.hs
module Main where
import Control.Concurrent
import Control.Monad

action :: [Char] -> IO ()
action xs = 
  forM_ xs (flip (>>) (threadDelay 500000) . putChar)

main = do
  forkIO $ action ['a'..'e']
  action ['v'..'z']
#+END_SRC

#+RESULTS:
>> avwbxcydze

\doublespacing

Our $action$ function takes a list of characters and slowly prints them. In $main$, we send one action applied to the list $['a'..'e']$ to a separate thread, and run the same action applied to a different list on the main thread. Because these actions are running concurrently we see the letters mixed with one another in the output (if we remove the $forkIO$, we would have seen 'abcdevwxyz' because the two actions run squentially).

The fact that 'w' appears before 'b' is surpsing, since we expected that 'a' appearing before 'v' implied that the first action's effects will happen before the second action's. In fact we have requested that the letters from the respective lists be printed approximately at the same time; the exact ordering depends on tiny fluctuations in the timing of threads in the runtime system. Running this progam repeatedly, 'b' does come before 'w', about half of the time. When the output of a concurrent program is noticeably different in circumstances that are not noticeably different, this is called a 'race condition' [@bernstein1966analysis]. The metaphor is that two threads are racing toward two positions in memory; the first one will write into the first slot and the second will will write into the second slot.

Does it matter whether 'b' or 'w' is printed first? The answer depends on the application. Even in moderately complex programs, predicting the locations of race conditions can be unintuitive and failing to notice them can result in corrupted data or a disruption of the forward flow of the program. In one famous case, a race condition among three power-line sensors lead to a failure of the alarm system at FirstEnergy Corp, preventing a response as snow-covered trees damaged a number of cables, [fn::  The report tells in fascinating detail just how hard routing out this kind of bug can be: ``The bug had a window of opportunity measured in milliseconds. 'There was a couple of processes that were in contention for a common data structure, and through a software coding error in one of the application processes, they were both able to get write access to a data structure at the same time,' says Unum. `And that corruption led to the alarm event application getting into an infinite loop and spinning ... This fault was so deeply embedded, it took them weeks of poring through millions of lines of code and data to find it.'"] and leading to a 2-day blackout for 8 million Canadians and midwesterners[@poulsen2004tracking].

As a rule of thumb, race conditions become problematic when mulitple threads share read and write access to common data, and some multi-step operations on that data only make sense if the operations are carried out without interruption. The classic example is an a pair of banks processing the transfer of money between ``Alice" and ``Bob" who live in two different states. In this case two different computers in two different banks need to access shared data: the balance of Alice's account and the balance of Bob's account. This pseudocode does the job:

\singlespacing

#+NAME: bankExample 
#+BEGIN_SRC haskell
module Bank.Transfer where

transfer :: IORef Account -> IORef Account -> DollarAmount -> IO ()
transfer fromAccount tAccount amount = do
  a <- readIORef fromAccount
  writeIORef fromAccount (a - amount)
  b <- readIORef toAccount
  writeIORef toAccount (b + amount)
#+END_SRC

\doublespacing

The $IORef Account$ type is a reference to a value that can be shared across threads. $transfer$ takes two references and an amount to transfer. It then reads the balances and increments each to reflect the money transfer. This works as long as Alice and Bob aren't at the bank at the same time. But if they try to send each other money at nearly the same time, this unfortunate sequence of events could happen:

#+CAPTION: \textbf{A race condition can corrupt data.} Two concurrent threads operating on shared data will often lead to a corrupted final state, because the individual steps in each operation change the shared data in ways not accounted for by the other thread. In this case, $10 have disappeared from the system after Alice and Bob make near-simultaneous transfers.
#+NAME:   tab:basic-data
| Time | Thread             | Command                   | Alice's $ | Bob's $ | Big problem? |
|------+--------------------+---------------------------+-----------+---------+--------------|
|    0 | Bob -($10)-> Alice | getBalance bob            |    100.00 |   50.00 | No           |
|    1 | Bob -($10)-> Alice | setBalance bob (50 - 10)  |    100.00 |   40.00 | Maybe...     |
|    2 | Alice -($20)-> Bob | getBalance alice          |    100.00 |   40.00 | Maybe...     |
|    3 | Alice -($20)-> Bob | setBalance alice (100-20) |     80.00 |   40.00 | Maybe...     |
|    4 | Alice -($20)-> Bob | getBalance bob            |     80.00 |   40.00 | Maybe...     |
|    5 | Alice -($20)-> Bob | setBalance bob (40+20)    |     80.00 |   60.00 | Maybe...     |
|    6 | Bob -($10)-> Alice | getBalance alice          |     80.00 |   60.00 | Maybe...     |
|    7 | Bob -($10)-> Alice | setBalance alice (80+10)  |     90.00 |   40.00 | Yes!         |

So functions that may be run in a concurrent context must somehow clamp the access to variables so that that threads don't make changes behind one another's backs. The basic tool for this is the /mutex/ [fn:: Mutex is short for Mutual exclusion], a `lock' that can be taken by at most one thread at a time. Mutexes are managed by the runtime system and are synchronized across all threads. It's up to the programmer's discretion how many she would like to create and which blocks of code she would like to wrap with them, and how tightly. But it turns out, there is no generally acceptable answer here either! We can wrap the entire $transfer$ function in a single mutex, but this means the entire program can only run a single transfer at a time; Bob and Alice are blocked from making their transfers while a completely unrelated transfer between Chris and Dean is going on. Alternatively, we could make a mutex for each account-holder in the bank, and in the process of transfering money between Alice and Bob, we take both mutexes in turn, perform the transfer, then return both mutexes. This can be made to work, but perhaps surprisngly, it is subject to a problem similar to that in the mutex-free case: the act of taking and releasign mutexes is prone to interaction effects. Alice transfering to Bob at nearly the same time that Bob transfers to Alice can cause one thread to take Alice's mutex just as the other take's Bob's, and then neither thread can make progress because each one is stuck waiting for the other to finish its transaction and release the complementary mutex. The topic is inherently difficult to understand, but the author is partly to blame for this too, in this case. For a more thorough treatment of this topic refer to Marlow's excellent Parallel and Concurrent Programming in Haskell [@marlow2012parallel]. The main point is, concurrency is often necessary to consider in programs that interact with the outside world, with inputs coming from multiple sources, and coping with it can be subtle and difficult.

*** Software transactional memory
Functional programming is a promising avenue for handling concurrency because so much of the business of functional programming is not about describing how things are done, but what things are. The relationships between values are timeless and constant. So the ``surface area" of a functional program on which parts are legitimately /changing/ is small. We still have to consider data races in those places where values change, and here Haskell offers an interesting solution, called Software Transactional Memory[@herlihy1993transactional] (STM) [fn:: Are my footnotes still working? ]. Allow concurrent mutation to happen without locks, and allow the runtime system to detect when this produces corruption, then undo whatever actions caused the corruption and run them again from a clean version of the data. This approach would be impossible, if the system were attempting to track the changes to every variable, and it is generally impossible to ``undo" arbitrary actions that change the state of the computer. But in Haskell we can declare a small subset of variables to be tracked by the system, and use the type system to enforce that the only variables allowed to change are those tracked ones.

Do we face the same challenge that we had with mutexes above, choosing between two unfavorable alternatives when in comes to the scale at which we wrap our actions with the change-tracking and rollback abilities? Not in such a strict sense, no. Single actions manipulate tracked variables, but the tracking and rollback by default do not happen. Instead of tracking by default, invidivual actions are able to be combined together with other actions, and the /composite/ action is tracked for corruption and allowed to be rolled back. In the bank example, we would combine the four-action sequence (get Alice's balance, set Alice's balance, get Bob's balance, set Bob's balance) into one composite action and mark that composite for tracking and rollback. This gives us the same level of protection as the single global mutex scenario. But individual threads are free to work simultaneously on other transactions.

This approach is very highly preferable to the premptive threading and mutex tools that programmers traditionally go to first when designing mulicore shared memory programs at this modest scale. But it does have a fairly serious drawback. Attempting to push a small number of expensive changes to shared variables from one thread, and a large number of very cheap changes from another thread, can cause the first thread to repeatedly fail to have its changes committed. This is known as /thread starvation/. What constitutes and expensive computation, a cheap computation, or a large number? These factors are system dependent. It can be hard to anticipate that thread starvation will rear its head. In general, one simply implements their algorithm using STM and tests to see that operation seems to be going as expected. But optimistically increasing the workload you subject an STM program to can be risky.

TODO: Get this into footnote. Other popular systems have implemented STM: the database management system PostgreSQL [@stonebraker1986design], Java, and the associated functional language Clojure[@menon2008practical], and it has been implemented at the hardware level[@hammond2004transactional]

* Coordinated information coding in a desynchronized network


** Abstract

Brain  areas involved in mnemonic and spatial processing are locked to an underlying 8-12 Hz oscillation known as the theta rhythm.  Different layers of entorhinal cortex, subcortical areas, and hippocampal subfields are each maximally activated during different theta phases; even within field CA1, theta-locked excitation is offset in a gradient manner.  In addition to pacing cell excitability, theta influences spatial information processing by organizing the timing of place cell ensembles into temporally precise sequences. We sought to determine the impact of theta timing offsets on the coordination of spatial representations in ensembles of place cells in different brain areas.

Along the CA1 septal-temporal axis, we found spatial information content to be synchronized to within 5 ms (TODO), despite a time offset in theta on the order of 30 ms (TODO). Offsets in excitability manifest as a subtle tendency for different brain areas to be more active at different phases of the theta cycle, but this is independent of the encoded spatial information. The same degree of inter-area synchrony is apparent in the hippocampus of stationary rats, which sporadically replays sequences of spatial locations.

This observed information synchrony in the context of desynchronized excitation provides a constraint for future models of fast-timescale space coding. Integrating these findings into a model of theta phase encoding can accoun for previous observations of diverse It also has implications for the integration of spatial signals in downstream structures, which may be befuddled by anything short of a coherent message from converging inputs.

This finding is at odds with prior models that strictly link theta phase to place cell spike timing. Adding a baseline excitatory drive to each area according to that area's phase offset brings the population information into synchrony and accounts for anatomical gradations in spatial receptive field shape.  These results show that fine-timescale information coding can be decoupled from underlying differences in timing of excitatory drive.


** Introduction

*** CA1 place cell excitation is timed by 10Hz oscillation - theta rhythm
In many brain areas associated with spatial learning [@morris1984developments; @o1971hippocampus; @ranck1984head; @hafting2005microstructure] and episodic memory [@milner1968further; @victor1971wernicke; @annese2014postmortem], neural activity is modulated by a 7-12 Hz oscillation called the theta rhythm [@vanderwolf1969hippocampal; @buzsaki2002theta; @mitchell1980generation].  The influence of the theta oscillation on spatial and mnemonic information processing has been appreciated at two levels. On the global level, theta is thought to coordinate activity between connected brain regions [@lubenov2009hippocampal; @jones2005theta; @sirota2008entrainment; @colgin2009frequency; @fries2009neuronal]. Locally, theta shapes the fine-timescale properties of information coding within brain areas, by way of theta phase precession [@o1993phase, @skaggs1996theta; @mehta2002role; @dragoi2006temporal; @hafting2008hippocampus].

*** Theta sequences
Place cells spike at precise phases of theta that depend on where a rat is within the place field. The spikes at the beginning of the place cell occur at early phases and spike near the end of the field occur at later phases [@o1993phase]. This lead to the prediction that a collection of cells with partlialy overlapping fields will fire in a strict order according to the relative positions of the place fields [@skaggs1996theta; @dragoi2006temporal]. Later these sequences were observed directly in large groups of simultaneously recorded neurons [@foster2007hippocampal; @gupta2012segmentation; @davidson2009hippocampal]. Interestingly, the tight temporal alignment of place cells in theta sequences is greater than the precision with which individual cells align their spikes to the theta oscillation [@foster2007hippocampal].

Our goal is to understand how the brain processes information during navigation and how that processing leads to later recall. Although we can determine a rat's position very precisely using only firing rate information from place cells [@ahmed2009hippocampal], it has not been established that this is how the rest of the brain interprets hippocampal spiking [@eichenbaum2000hippocampus]. The conspicuously precise sequential ordering of place cell ensembles beyond the temporal resolution of a rate code suggests these sequences as not just a means for adding a bit more accuracy to an estimate of one's current position [@maurer2007network] but a potentially fundamental aspect of limbic information processing [@cheng2013rigid; @chen2014neural].

Understanding the mechanism of theta sequence generation will be important for reasoning about their interactions with other brain areas and possible functional roles. There is not agreement about their mechanism though (in fact very little is known about the origins of spatial properties of cells throughought the limbic circuit), but hypothesized models fall into three camps that emphasize either oscillatory interference, Hebbian phase sequences, or subthreshold receptive fields.

The oscillatory interference model suggests that place cells oscillate at a rate slightly faster than theta measured in the LFP, and that the interference pattern of the two oscillators is a more complex waveform with local peaks that precess with respect to the LFP theta and a low frequency envelope that determines the length of the place field (for details, see [@o1993phase]). This theory is supported by intracellular recordings of the soma and dendrites of hippocampal place cells, which do indeed produce precessing spikes in response to inputs presented at different frequencies [@magee2001dendritic], and which show some signs of oscillatory accelleration during movement through the place field [@harvey2009intracellular]. The main weakness of the model is that the oscillations discussed are purely temporal and their interaction is expected to carry on at a constant rate regardless of an animal's speed, but in reality phase precession aligns with the animal's location within a place field better than it does with time spent in a place field [@o1993phase]. Tweaking the model to match this observation requires a strong correlation between running speed and theta frequency that isn't seen empirically [@o1993phase; @mcfarland1975relationship; @slawinska1998frequency]. An additional problem for the oscillatory interference model comes from the observation that phase precession continues after the cessation of a brief silencing of the hippocampus [@zugaro2004spike; @moser2005test].

Hebbian phase sequences [@hebb2002organization] are sets of assemblies of cells that excite one another in a feed-forward synaptic chain. One constellation of neurons preferring location $x$ on the track collectively synapse on the population preferring $x+\delta$, and so on, causing a rapid succession of cell assemblies, initiated by the sensory details of the current physical location and terminated by the subsequent surge of theta-rhythmic inhibition, to sweep out a theta sequence ahead of the rat. This model suffers when it comes to producing theta sequences in two-dimensional arenas. Empirically, theta sequences sweep forward in the direction that the rat is facing. The hippocampus would somehow need to unmask selectively the synapses that activate a West-bound sequence of assemblies when the rat is headed West and a North-east bound sequence when the rat is headed North-east, etc. At this point it becomes hard to imagine how such specific matrices of connections could be formed and subsequently selected on a moment-to-moment basis, although there is some work involving grid cells [@hafting2005microstructure] that may make this more feasible [@mcnaughton2006path].

The third model, called the rate-to-phase model, considers CA1 spike timing as an interaction between an excitatory input that ramps up smoothly with spatial location in a place field, on one hand, and the temporally oscillating inhibitory effect of theta, on the other [@mehta2002role]. For any given position in the place field, spikes are fired at the moment when inhibition drops below the excitation associated with that location. Progress in the field and greater excitation means less waiting for inhibition to drop to meet the excitation level, and thus earlier phases. This model directly references space to achieve phase precession, and therefore naturally copes with the finding that phase precession goes according to distance traversed within a field than time spend in a field. Because neither this model nor the phase sequence model involve previous reverberatory activity, they are both compatible with the transient inhibition studies [@zugaro2004spike; @moser2005test].

One thing that is unclear about the rate-to-phase model is where these hypothesized excitatory ramp comes from. The ramping excitation is strictly required to be monotonically increasing; if it were bell-shaped like the place fields of some place cells, we would expect to see a pattern of phase precession followed by phase procession back to late phases, but this does not happen. Additionally, there is no known mechanism that could smooth the summed inputs to CA1 into such a flat ramp; indeed the main inputs to CA1 are themselves theta-rhythmic signals.

Another parameter the rate-to-phase model leaves abstract is the nature of the rhythmic inhibition. It it the somatic inhibition on place cells, the trough of dentricic excitation, the peak spiking phase of a particular class of interneuron? The model works well for accounting for phase precession through a single field without committing to a particular concrete source of theta, but we will need more information as we try to predict higher-order features, for example, the behavior of pairs of cells that for whatever reason are not receiving identical inhibitory inputs.

Theta phase precession has been observed in CA3 [@o1993phase], dentate gyrus (DG) [@skaggs1996theta], medial prefrontal cortex [@jones2005phase], ventral striatum [@van2011theta], entorhinal cortex (EC) layer II (but not layer III) [@hafting2008hippocampus] and the subiculum [@kim2012spatial], as well. Whether phase precession organizes cells within a given area into theta sequences or supplies some other form of ensemble organization, we refer to this as a 'local' role for theta: a set of timing constraints that single cells or small groups must obey perhaps for temporal coding or time-sequence coding.

*** Communication through coherence
At the same time, there is an effort to understand the firing properties of parts of the hippocampus in terms of information flow through a heirarchy of structures with unique functions, similar to the work being done in the visual circuit [@maunsell1983connections]. Here, theta is hypothesized to have a more global role in facilitating the transmission of information between areas. Hippocampal and prefrontal cortical theta oscillations become coherent at times when maze navigation needs to refer to the contents of working memory [@jones2005theta], and this coherence is reflected in the spike times of both hipocampal and prefrontal neurons [@siapas2005prefrontal; @wierzynski2009state]. Spatially restricted bouts of gamma oscillations in somatosensory cortex are modulated by hippocampal theta phase [@sirota2008entrainment], providing an interesting link to the large litterature surrounding cortical gamma oscillations. More direct evidence on the role of theta in pacing gamma oscillations comes from Colgin et. al. [@colgin2009frequency], who showed that in CA1, high-frequence gamma oscillations occur primarily at the peak CA1 spiking phase and are coherent with simultaneous high-frequency gamma oscillations in entorhinal cortex; while low-frequncy gamma is strongest about 90Â° earlier and is coherent with low-frequency gamma in hippocampal CA3. Based on this Colgin argues that selective theta coherence tunes CA1 to comminucate prefferentialy with one input source or the other.

A related global role for theta attributes specific sorts of information processing to different theta phases; namely that the phase associated with peak entorhinal cortex spiking is responsible for encoding new information and the phase associated with input from CA3 is responsible for memory retrieval [@hasselmo2002proposed].

*** Tension between hypothesized roles in gating communication channels and encoding
These two roles for theta oscillations are difficult to unify, because they make conflicting demands on the details of how neurons interact with the oscillation. If a cell is meeting the timing requirements of selective communication with varying brain areas, can it simultaneously be aligning its spikes fall at progressively earlier theta phases when a rat moves through the cell's place field? 

One model suggests that meeting these requirements simultaneously results in strict relationships between global and local phenomenon, and that we get scientific findings from this 'for free'. Fries [@fries2009model] extrapolates from Colgin's [@colgin2009frequency] work and concludes that early phases of theta in CA1 processes cortical information about the current state of things, and later phases use the modeling capabilities of CA3 to extrapolate into the future. This model is appealing when observing the shape of theta sequences in place cell ensembles; they begin near the rat's current position and sweep quickly out ahead of him, repeating this at every theta cycle. And it accords with the notion of the entorhinal cortex as a sensory structure (being upstream of the hippocampus), and Marr's notion of the CA3 as a pattern extrapolator [@marr1971simple].

Can we account for the theta-locked spike timing of limbic circuit structures in terms of their anatomical ordering? Here things become more difficult. Mizuseki et. al. [@mizuseki2009theta] show that cells in different structures prefer to fire at differet theta phases that bear little reseblance to the sequence implied by a synaptic traversal of the circuit. Rather than EC-Layer II $\rightarrow$ DG $\rightarrow$ CA3 $\rightarrow$ CA1 $\rightarrow$ -> EC-Layer V (the synaptic pathway of the major hippocampal excitatory circuit), CA3 principal cells to spike 90Â° earlier than EC-Layer II principal cells.  Additionally, spikes of CA1 neurons occur at the opposite phase from that of their peak dendritic excitation [@kamondi1998theta; @buzsaki2002theta].  This phenomenon is acceptable to the global account of theta; it allows for the opening of 'temporal windows' of processing between sequential anatomical processing stages [@mizuseki2009theta]. But it is at odds with intuitive and formal [@huxter2003independent; @kamondi1998theta] models of the fine timescale spiking of place cells, which we expect to follow behind their inputs by conduction delays and synaptic delays. The empirical timing relationships are much longer [@mizuseki2009theta].

Colgin et. al. present data in support of a model associating particular phases of the theta oscillations of CA1 with the opening of specific communication channels to either CA3 or the entorhinal cortex [@colgin2009frequency].  The tension between theta's local and global roles is apparent here, as well. To the extent that CA3-CA1 and entorhinal-CA1 communication is limited to narrow windows of theta phase. Contrary to this, place coding in CA1 involves a smooth transition through cell ensembles that extends over much of the theta cycle [@foster2007hippocampal; @gupta2012segmentation].


*** Theta as traveling wave, excitatory time offsets over hippocampal CA1
Lubenov and Siapas [@lubenov2009hippocampal] presented a novel finding about the nature of the theta oscillation itself. Using large grids of tetrodes carefully positioned a uniform distance from the hippocampal cell layer, and sampling a large extent of the length of the hippocampus, they showed that the theta rhythm is not synchronous within hippocampal CA1. Instead, theta at the septal pole of CA1 are advanced in phase, theta in more posterior parts of CA1 are phase delayed, and theta measured inbetween has a graded delay. The combined activity of these delays resembles a traveling wave with a peak of excitation that 'moves' down the hippocampal long axis once for every cycle of theta. These findings were extended beyond dorsal hippocampus to the entire length of CA1 by Patel et. al. [@patel2012traveling].

By fitting a traveling wave model over the pattern of theta offsets observed over many tetrodes, Lubenov and Siapas were able to extract parameters that can be used in concrete hypotheses. The characteristics of the wave vary from cycle to cycle, but tend to have a spatial wavelength of 12mm, a wavefront speed of 75 mm/sec and a preferred direction about half way between the medial-lateral axis of the skull and the septo-temporal axis of the hippocampus. Based on these parameters [@lubenov2009hippocampal] and our own LFP measurements, we can establish the mean expected time offset along the direction of wave propagation as $1/\nu$, 12.8 $\pm$ 3.2 ms per mm.

*** Theta sequences: locally paced or globally synchronized?

The view of theta as a traveling wave will need to be factored in to any future models that unify the local and global roles for theta, because it has interesting implications in both areas. With theta mediating information transmission to and from CA1, how will those inputs and outputs cope with the fact that the window of receptivity is a moving target? Is it acceptable that structures receiving inputs from one part of CA1 will see maximum activity at a different time from structure receiving inputs from another part of CA1 - and could this sequencing actually be useful? 

How does this fit when we zoom in from talking about bulk spiking rates, to the level of information-carrying single spikes at the local level? If theta phase precession conforms to the anatomicaly sweeping of peak excitation, then theta sequences composed of sets of cells from different regions of CA1 would be similarly offset in time. The periodic replay of spatial sequences would begin slightly earlier in septal CA1 ensembles, and ensembles near intermediate CA1 would begin the same sequence about 45ms later, with ensembles further posterior starting later still. This time shifting may seem to complicate attempts to square theta sequences with anatomical communication. However, it leads to an interesting prediction: that local regions of hippocampus begins a representation trajectory at offset times. Because of this, a downstream structure observing a snapshot of the spiking activity across the whole hippocampus would see different parts of the track encoded at different anatomical locations. Or as Lubenov and Siapas put it, the hippocampus at any instant would not represent a point in space, but a linear span in space [@lubenov2009hippocampal].

Alternatively, theta sequences may not conform to the timing offsets suggested by the traveling theta wave, and the encoded information may be temporally synchronized over large anatomical distances, despite the presumed timing differences in their underlying drive. This scenario presents a very different picture to downstream structures - one in which bulk spike output of the hippocampus goes as a traveling wave, but the information content within it is coherent, and the entire structure does agree to a single point on the track at any instant.

We set out to measure the timing relationship between theta waves and place cell sequences in order to address this one question among many others aimed at unifying the local and global roles for theta in timing spikes. We characterized the impact of spatial tuning and anatomical distance on the cofiring of pairs of place cells, as well as the timing relationships of population-encoded trajectories recovered from anatomically distinct groups of cells, both across CA1 and between CA1 and CA3. We found that in most cases, timing offsets in theta sequences were significantly more synchronized than the temporally offset excitatory waves that modulate them. We suggest that information synchrony may be decoupled from the mechanisms that modulate excitation. This decoupling could be achived in a trivial way, by stipulating that phase precession begins and ends according to an underlying source that is in fact synchronized across hippocampus; or it could be achieved through an active mechanism that supplies extra excitation to the regions that would otherwise be temporally delayed by the traveling theta wave.

** Materials & Methods
*** Subjects

All procedures were approved by the Committee on Animal Care at Massachusetts Institute of Technology and followed US National Institutes of Health guidelines. Tetrode arrays were assembled and implanted  according to the procedure in Nguyen et. at. (TODO 2008) and Kloosterman et. al (TODO 2008). We made several modifications to the materials and proceduces to improve our multi-cell sampling.  First, we glued several hundred half-inch pieces of 29 guage and 30 guage hypodermic tubing into rows about 6 mm long, then stacked and glued the rows together to form a honeycomb patterned jig, for organizing the tetrode guide-tubes that would eventually inhabit the microdrive. Second, we developed the ArtE recording system (TODO detailed in Chapter 2) to run in parallel with our usual usual tetrode recording rig. The broader goals of the ArtE project are to enable real-time data analysis and feedback, but in this experiment we used it merely to increase the number of simultaneously recorded tetrodes.

*** Single-unit tetrode recording

Microdrive arrays were implanted with the center of the grid of tetrodes overlying dorsal CA1 (TODO A/P -4.0, M/L 3.5), spanning 3 mm of hippocampus in the septotemporal dimension and 1.5 mm proximo-distal. In two rats (TODO correct?), tetrodes were lowered into the pyramidal cell layer of CA1 over the course of 2 to 3 weeks and left there for several more weeks of recording.  In two more rats, tetrodes were first lowered into CA1, and later a subset of those was moved further to record simultaneously from field CA3. In each cell layer, we sought to maximize the number of neurons recorded and to minimize within-experiment drift, so closely tracked the shape of sharp wave ripples (which undergo characteristic changes during approach to the cell layer) and later the amplitudes of burgeoning clusters. If either of these factors changed overnight to a degree greater than expected, the tetrode was retracted by 30 - 60 micrometers.

*** Behavioral training

Behavioral training began when nearly all tetrodes exhibited separable spike clusters, and consisted of rewarding rats for simply running back and forth on a curved 3.4 meter linear track, or running continuously clockwise on a 3.4 meter long circular track, with rewards given for every 360 degrees of running for the first 3 laps and for every 270 degrees thereafter. Food deprivation began one or two days prior to the beginning of acquisition, with rats receiving 30 grams of food per day, adjusted up or down depending on the rat's motivation to run and level of comfort (assessed by the amount sleep taken before the running session). The target food-deprived weight was 80\% of free-feeding weight, but we rarely achieved this without disrupting the sleep of the animals, so body weights tended to be 90\% of the free-feeding weight or more, especially after rats learned the simple rules of the task. Additionally, we provided large rewards throughout training (2-5 grams of wetted powdered rat chow per lap), to encourage the long stopping periods during which awake replay can be observed (TODO Foster, 2006). Under these conditions, rats run for about 20 laps or 30 minutes before becoming satiated and ignoring rewards.

*** Electrophysiological Characterization
Spikes and local field potentials were voltage buffered and recorded against a common white-matter reference, at 32 kHz and 2kHz respectively, and position was tracked at 15 Hz through a pair of alternating LED's mounted on the headstage, as in (TODO) Davidson et. al. (2009). Spikes were clustered manually using the custom program, xclust3 (M.A.W. TODO). Place fields were computed for each neuron as in Brown Sejnowski et al (TODO), by partitioning the track into 50 to 100 spatial bins, and dividing the number of spikes occurring with the rat in each spatial bin by the amount of time spent in that spatial bin, in each case only counting events when the rat was moving at least 10 cm/second around the track. Direction of running was also taken into account, allowing us to compute separate tuning curves for the two directions of running, which we label 'outbound' and 'inbound'.

To characterize the phase differences among tetrodes in CA1, a simple spatial traveling wave model was fit to the theta-frequency filtered LFP signals and the theta-filtered multiunit firing rate in turn, as in Lubenov and Spapas [@lubenov2009hippocampal]. TODO expand on this.

*** Theta sequences
Two complementary techniques were used to assess the relationship between phase offsets between tetrodes and timing offsets in spatial information encoding. First, in CA1-only recordings, a pairwise regression was performed similar to that in Dragoi and Buzsaki (TODO 2006), measuring the dependence of short-timescale peak spike time differences on the distance between the peaks of that pair's place fields. We added a second inedpendent variable to this regression: the anatomical distance between each pair of place cells. The result is a model that predicts the average latency between any pair of cells, given that pair's place fields, that pair's anatomical separation, and the parameters of the traveling wave pattern of phase offsets.

Second, Bayesian stimulus reconstruction (TODO Zhang et. al., 1998) was carried out independently using place cells from thre groups of tetrodes at the most septal end, the middle, or the most temporal end of the 3mm recording grid. Unlike the case for large populations of neurons, reconstructions from smaller anatomical subsets are considerably more noisy and do not reliably yield theta sequences. Session-averaged theta sequences were recovered by aligning the reconstructed position according to a shared theta phase and the rat's position on the track at that time. In both raw and session-averaged reconstruction cases, 2d autocorrelograms were taken to quantify the time-delay and space-delay between pairs of tetrode subgroups.

** Results

*** Theta phase spatial properties and timing offsets: 13ms delay per mm
We first characterized the timing of the local-field potential (LFP) theta rhythm within a ~3mm long, 1.5mm wide strip dorsal CA1, in electrodes embedded near the pyramidal cell layer. A traveling wave model was fit to the theta-filtered and Hilbert-transformed signals from 16 to 24 tetrodes, in 0.25 second segments, resulting in a timecourse of traveling theta wave parameters (Figure 1 TODO link). We focus on the parameters that characterize the desynchronization: spatial wavelength, wave propagation direction, and temporal wavelength. 

#+CAPTION: \textbf{Theta is desynchronized within CA1.} \textbf{A.} The rat hippocampus (left dashed region) occupies a large of the cortex. Three example recording sites (red, green and blue points) experience different phases of theta oscillation in the local field potential (right). On average, recording sides experience inreasing phase delay as they move lateral and posterior [@lubenov2009hippocampal]. Raw LFP traces (in grey) exhibit theta oscillations and gamma oscillations that depend on electrode depth. Filtered theta components shown in red, green, and blue. \textbf{B.} The pattern of phase-offsets in LFP recordings was fit by a traveling wave model in 0.25 second segments. The traveling wave model consists of parameters for wave direction ($\theta$), spatial wavelength ($\lambda$), amplitude (A), temporal frequency (f), and phase offset ($\varphi$, not shown).
#+NAME:   fig:SED-HR4049
[[./finalFigs/brainAndModel.png]]


These parameters vary on a short timescale but are fairly consistent between animals when averaged across time. Theta frequency during running varies from 8.2 Hz $\pm$ 0.5 (mean $\pm$ standard deviations). The spatial wavelength is 6.3 mm $\pm$ 3.6 after removal of outliers, and the dominant propagation direction is 18Â° anterior to the medial-lateral axis. Surprisingly, the fit of the model was not higher during running than during stopping periods when theta amplitude is low, suggesting that traveling waves are a broad enough family to fit many patterns of data (in fact, a traveling wave model will perfectly fit a set of perfectly synchronized oscillators; the spatial wavelength in this case would be infinity). As was previously reported [@lubenov2009hippocampal], proximity of tetrodes to the pyramidal cell layer obscures the LFP measurement of the traveling wave, so we primarily rely on previously reported wave parameters.

#+CAPTION: \textbf{Theta traveling wave parameters can be stable over time}. \textbf{A.} About of running (green time window, top panel) elicits a stabilization of the traveling wave fit to theta. During this time, the spatial wavelength varies between 3 and 10 mm, and wave direction remains fairly constant about 18Â° anterior to the medial-lateral axis, except for a brief direction flip near the end of the run. The fit of the model to the data is not better during the running periods than the stopping periods, although the variability in parameters during run is lower, because the ability to record the traveling wave is lower when the tetrodes are near the pyramidal cell layer [@lubenov2009hippocampal], as they were in this case. \textbf{B.} The traveling wave velocity inverted gives a wave-delay interval. In this dataset, the mean delay was 17.8 ms per mm along the medial-lateral axis. Combining with studies optimized for recording the LFP, the estimate is 12.8 ms per mm.
#+NAME:   fig:waveParameters
[[./finalFigs/waveTimecourse.png]]

 
*** Place cell pairs are synchronized across anatomical space
We directly measured the relationship between anatomical spacing and spike timing in pairs of place cells. If two cells with the same place field and phase precession profile are separated by a spatial interval corresponding to a 13ms delay between theta peaks, two fast-timescale timing relationships are possible. Either phase precession is locked to to the local theta oscillation, and spikes from the cell 1mm 'downstream' with respect to the traveling wave will occur 13ms later than those of the upstream cell (Figure 3). Alternatively, if phase precession disregards the anatomical delays of theta phase, then spikes from the two cells should fire roughly in synchrony. Other timing relationships are possible of course, but it is not clear what they would imply mechanistically.

#+CAPTION: \textbf{Assessing the effect of tuning curves and anatomical location on spike timing.} \textbf{A.} Three place cells with partly overlapping fields. Tuning curves are plotten in the left column. In the center and right columns are a raster plot of several seconds, and a several hundred millisecond detail. \textbf{B.} Cell A's place field peak is 25 cm beyond Cell B's, its anatomical position is 0.66 mm more lateral, and it tends to fire 74 ms later (the peak offset of the cross correlation of the two spike trains). Cell A's place field peak is 7 cm beyond Cell C's, its anatomical position is 1.29 mm more lateral, and it tends to fire 24 ms later. \textbf{C.} A scatterplot of all pairs of place fields (gray dots), taking the peak time offset between the spike trains as a function of both place field distance and anatomical distance. Projecting all of the points to one axis shows the correlation between field distance and time offset due to theta sequences (blue). Projecting onto the other axis shows the much weaker correlation between anatomical offset and timing offset (red).
#+NAME:   fig:pairsExplanation
[[./finalFigs/pairXCorr.png]]

These predictions can be generalized beyond cell pairs with perfectly overlapping fields. Field separation will result in a timing shift due to phase precession. The virtual speed of the rat encoded in theta sequences is about 10 m/s, so a cell with a field peaking 0.5 meters beyond that of another cell will tend to spike 50 ms later. If phase precession is paced against the local theta, then anatomical separation on the axis of the traveling wave should add to this delay linearly. We can estimate the effects of place field spacing and anatomical spacing on spike timing by linear regression (Figure 3).

#+CAPTION: \textbf{Field location is the primary determinant of spike time offsets.} The scatterplot of the previous figure, combining all cell pairs (gray) from four recording sessions, considering timing offsets (z-axis) as a function of both place field distance (right axis) and anatomical distance (left axis). Projecting the points onto one axis shows a strong correlation between field distance and timing offsets (blue) due to theta sequnces. Projecting onto the other axis shows the much weaker correlation between anatomical offset and timing offsets (red).
#+NAME:   fig:pairsSummary
[[./finalFigs/pairXcorrSummary.png]]

Pooling cell pairs across rats, we estimate each meter of place field distance to contribute 147.4 $\pm$ 14.2 ms of delay and each mm of anatomical spacing along the traveling wave axis contributing 0.7 $\pm$ 3.3 ms, significatnly less lower than the expected 12.8 ms per mm (p < 0.05). In other words, place cells fire with temporal delays that reflect spatial relationships on the track, and these spiking events are tightly coordinated throughout the measured extent of CA1 (about 3 mm).

#+LaTeX: \pagebreak[3]

#+CAPTION: \textbf{Anatomical separation accounts for relatively little timing offset.} Results of the regression analysis of the previous figure for each recording session. In three of four rats, the isolated effect of anatomical distance of time offsets is less than the 12.8 ms per mm time delay of the theta wave. Pooling cell pairs into a single regression results in a final estimate of 0.7 ms per mm. The effect of field separation on the other hand is reliably in line with previous accounts of theta sequences.
#+NAME:   tab:basic-data
| Session   | Anatomical (ms/mm) | Field (ms/m)     | Offset (ms)     | # of Pairs  |
|---------  | ---------          | -----            | -----           | ----        |  
|Yolanda A  |  -7.0 $\pm$ 13.9   | 101.2 $\pm$ 20.0 | 7.3 $\pm$ 11.3  | 31          |
|Yolanda B  |  -1.2 $\pm$ 16.4   | 199.1 $\pm$ 40.9 | 1.2 $\pm$ 11.0  | 18          |
|Morpheus   |  0.9 $\pm$ 3.3     | 163.1 $\pm$ 20.7 | -2.1 $\pm$ 5.2  | 38          |
|Caillou    |  18.6 $\pm$ 12.8   | 198.1 $\pm$ 25.1 |  6.4 $\pm$ 7.9  | 19          |
|\textbf{Total} |  \textbf{0.7 $\pm$ 3.3} | \textbf{147.4 $\pm$ 14.2} | \textbf{-0.4 $\pm$ 3.5}  | \textbf{106} |


*** Ensemble theta sequences are synchronized
To assess the impact of anatomical distance on spatial representations from another angle, we turned to population decoding, which provides a direct view of theta seuences as well as spontaneous spatial replay events. 

#+CAPTION: This is the caption for the next figure link (or table)
#+NAME:   fig:sequences
[[./finalFigs/sequences.png]]

Within CA1, we partitioned cells into three groups according to the tetrode they were recorded on, then discarded the middle group, leaving two groups separated by a millimeter at their closest point, two millimeters on average. We then reconstructed the rat's location twice, once from each set of tetrodes, at a 15ms temporal scale suitable for observing theta sequences. The division of tetrodes into independent anatomical groups drastically degrades the appearance of ongoing theta sequences, because the reconstruction process at such short timescales requires input from a large number of neurons. But clear theta sequences can be recovered by combining segments of the position reconstruction, aligned in time by peaks of the theta rhythm, and in space by the rat's current track position (Figure 5B).

#+CAPTION: \textbf{Theta sequence cross correlations for all recording sessions.} The cross correlations between theta sequences computed from medial and lateral place cell groups for each recording session. Diagonal streaks across the origin indicate that theta sequences resemble one another after a combination of time shift and position shift, but not a time-shift alone. Black arrows: pure time shift between medial and temporal cell groups. White arrows: time shift expected if theta sequences are uniformly delayed by traveling theta wave.
#+NAME:   fig:sequences_all
[[./finalFigs/sequences_all.png]]


We asked whether the reconstructed theta sequences are aligned with one another in time by taking their two-dimensional cross-correlation (Figure 4). A uniform delay of the theta sequence by time $\delta$ would appear as a diagonal streak that crosses the x axis at $\delta$. The peak of this cross correlation occurs when septal CA1 leads temporal CA1 by 3.5 ms in time. We estimate the uniform delay that theta sequences would incur in the lateral portion of the hippocampus by multilying the 12.8 ms/mm delay estimate by the mean inter-place cell distance for each recording session. In this example, the mean spacing between place cells is 1.05 mm along the medial-lateral axis, so a simple delay would result in 13.44 ms. These statistics for each rat are given in Figure 6 and Table 2. Observed time offsets are significantly different from those expected by uniform time delay of the traveling wave (p < 0.05).

In two of the recording sessions, although the cross correlation extends through the origin, its center of mass is delayed. This pattern indicates that lateral cells fire tend to fire later than medial cells, but with a balanced advancement in encoded track location. Rather than theta sequences being time delayed in lateral cells, the bulk of the spiking involved in a theta sequence comes from medial hippocampus first, and from lateral hippocampus slightly after, with the infomation content between them closely coordinated. This pattern is in the opposite direction in the two other recording sessions; indicating that on average in those two sessions, lateral hippocampus place cells fire more vigorously in the first half of theta sequences and medial hippocampal place cells fire later. Collecting all of our datasets, we do not find signifigant evidence to reject the null hypothesis that the cross-correlation center of mass is at zero (p = 0.7030), but we suspect that this is due to the low number of recording sessions and the dependence of this measure on the number of place cells simultaneously recorded.


#+CAPTION: \textbf{Theta sequences are aligned in time.} Averege and minimum anatomical distances, along the direction of wave propagation, between place cells in the two groups used for theta sequence decoding. Expected time offsets are derived from the estimated wave delay times the mean spacing. Offsets observed from the mean theta-sequence cross-correlations are close to zero, suggesting that that distant theta sequences are tightly synchronized.
#+NAME:   tab:basic-data
| Session   | Mean spacing (mm) | Min spacing (mm) | Expected offset (ms) | Observed offset (ms) |
|---------  | ---------         | -----            | -----                | ----                 |
|Yolanda A  |  1.33             | 0.94             | 17.02                | -4.0                 |
|Yolanda B  |  1.30             | 0.94             | 16.64                | -3.0                 |
|Morpheus   |  2.27             | 1.47             | 29.06                |  6.0                 |
|Caillou    |  1.05             | 0.69             | 13.44                |  3.5                 |


** Discussion

*** Theta traveling wave matches previous report: ~20ms/mm delay

Within CA1, theta oscillations are offset in time along the medial-lateral axis. Previous studies of theta oscillations generally rely on the simplifying assumption, justified by experimental evidence at the time [@bullock1990coherence], that theta within the CA1 pyramidal cell layer is synchronized. The fact that it is not synchronized means questions like ``What is the phase offset between CA3 and CA1'' (for example) are no longer well-defined. For any claim about CA1 theta phase, we must specify exactly which part of CA1 we are talking about, or else specify that we are describing a process that is globally synchronized and therefore acts independently of local theta phase offsets.

*** Despite theta timing differences, information coding is synchronized
We reevaluate place cell's spiking relationship to theta in this context of unsynchronous theta. First we show that theta sequences, chains of place cell firing throught to be coordinated through their tight coupling to theta phase [@mehta2002role], are tightly synchronized with each other, in spite of the desynchronization of the underlying theta rhythms. This information content synchronization exists between subsections of CA1 that differ in theta timing by by 10 to 25 ms.

*** Different parts of CA1 may weakly preferentially carry most of the spike rate at different times 
In Mehta & Wilson's [@mehta2002role] model, inhibitory theta oscillations control the timing of place cell spikes in theta sequences. We interpret the traveling LFP theta wave as a desynchronization of that inhibition. A similar gradient of phase offsets is seen in the multiunit firing rate (although with slightly different wave characteristics and a less clean spatial correlation) [@lubenov2009hippocampal]. So if theta is desynchronized within CA1, how can theta sequences there be synchronized? Is this a contradition in terms?

If it seems that a traveling theta and theta-locked phase precession strictly imply that theta sequences should be desynchronized, this may be due to the accidental adoption of definitions of terms that mean more than what was shown in the original findings that supported them. For example, consider the finding that phase precession begins at peaks of theta recorded on the same tetrode and precesses backward toward the trough [@o1993theta; @skaggs1996theta]. It is easy to confuse an incidental fact (that theta was recorded from the same tetrode as the place cell) with the essectial fact (that spikes precess to earlier phases). In fact, whether phase precession begins at the peak of /local/ theta (which is now known to not be synchronized across CA1), or begins simultaneously for all place cells (so, at /different/ local theta phases) is closely related to the empirical question that we tested in this paper.

A simple alternative account of the fact that place cells themselves have different preferred firing phases in different parts of CA1 is that different parts of CA1 preferrentially contribute to different parts of a theta sequence, although the spatial content of these sequences is temporaly aligned. The trumpets section is louder than the violins in the second measure; that doesn't imply that trumpets and violins play the same song but trumpets started one measure later. This pattern of synchronized content but desynchronized participation should be visible in the cross-correlations of theta sequences from different parts of CA1 - the region of good time matching should be a streak that goes through the origin, with a center of mass that is ahead of the origin. We failed to find experimental support for this pattern (Figure 6). However we expect that this is due to the large dependency of theta sequence decoding on large numbers of simultaneously recorded place cells, and that we can only definitively assess this model with better recordings of more cells.


    
*** Model 1: Spatially graded, temporally constant compensating excitation

First, we propose a gradient of additional baseline excitation, greatest at the lateral pole of CA1 and least at the medial pole. Because spike times are locked to the moments when input excitation overcomes thata-rhythmic inhibition, extra excitation shifts these times to earlier phases. Applying greater excitation at points where theta is phase delayed would bring those otherwise-delayed spikes back into alignment with medial place cells, which experience less phase delay.

This model is not especially parsimonious, but it does make an testable prediction, which is borne out in the data. Under the excitatory input gradient gradient model in Mehta and Wilson [@mehta2002role], additional uniform excitation should expose a greater extent of the subthreshold receptive field, resulting in longer place fields with more spikes in the 'anticipatory' part of the field and greater field asymmetry. 

*** Model 2: Phase precession inherited from synchronized afferents

An alternative account for synchronized theta sequences throughout CA1 can be built around a less literal coupling between theta oscillations and phase precession. In this model, CA3 and entorhinal cortex (two of the known spatial-information carrying inputs to CA1) are modulated by a theta rhythm that is uniform within each respective area - the traditional view [@mizuseki2009theta]. Theta recorded at any given point CA1 is inherrited from both of these areas and appear as a mixure of the two, in proportion to the relative strengths of the afferents at that point. But rather than organizing according to this local, mixed theta, CA1 spikes inherret their precise spike times directly from the spikes of the upstream brain areas. Without a traveling wave in CA3 or entorhinal cortex, all CA3 phase precession is synchronized and entorhinal cortex phase precession is synchronized; and for the sake of the model, CA3 phase precession is synchronized to entorhinal cortex phase precession. Now, the spikes of CA1 cells that are the result of either CA3 or entorhinal cortex input are aligned with respect to the spatial locations that the input units represent. What is offset in time is the phase-dependent modulation of spiking probability. Whatever the track position-by-phase relationship of a place cell, different phases of theta are associated with higher or lower spiking rates. In CA3, spike rates are higher during earlier phases of theta, and entorhinal cortex cells express higher firing rates at later phases.

This model accords with our findings in measuring place-cells: theta-timescale shifts in population firing rate, but maintained synchrony of the underlying information content. We shed the assumption of a perfectly balanced compensating excitation from the previous model, but pick up a new assumption: that positional information in entorhinal cortex is synchronized with that in CA3. This claim lacks empirical backing, and in fact it's not clear that such a timing comparison could even be made, because spatially selective neurons in entorhinal contex are grid cells [@hafting2005microstructure], not place cells. However, theta phase precession is present [@hafting2008hippocampus] in most layer 2 entorhinal grid cells (these project mainly to CA3), but only sparsely in layer 3 grid cells (which project to CA1). 

This does not necessarily contradict the Mehta and Wilson (2002 TODO) model, in which theta phase is not concretely linked to the spike times of any particular local group of neurons. On the contrary, if the inhibitory oscillation that paces place cell spikes is derived from a single source like the GAGAergic theta cells of the septum [@petsche1962significance], then phase precession synchronization across brain areas would be expected, despite differences in the phases of the field potential theta recorded in those areas. 


*** Information timing is decoupled from bulk firing rate for globally coherent coding

The main contribution of this paper is the finding that theta sequences as we understand them are impressively highly synchronized (to within less than 10 ms) across large expanses (3 mm) of the hippocampus, and that this synchronization is achieved in a context of desynchronized rhythmic firing. This finding raises questions about which of the above explanatory models (or an entirely different model) is responsible for establishing this synchronization. We of course also want to know whether this rule is true of the remaining 7mm of CA1, the most remote end of which is thought to express some emotional content in favor of seemingly arbitrarily-chosen spatial locations [@royer2012distinct]; and we would like to know whether theta sequences in upstream areas are synchronized with those of CA1, or lead it by one or two synaptic delays.

Theta sequenceses appear promising as a foundatation for an account of how the hippocampus encodes spatial, mnemonic, and sequential information. But it is important to point out that our understanding of the theta sequence as we describe it now is still tainted by a faulty definition. We define theta sequences as the ordering of spikes from place cells in terms of the relative positions of their /peak firing rates/. 

This is problematic. For example, how would our results differ if, unbeknownst to us, some place cells encode where the animal /will be/ in the near future rather than where the animal is now? There is experimental evidence that this is in fact the case [@wood2000hippocampal; @ferbinteanu2003prospective; @ji2008firing], and that failing to account for it degrades the quality of position decoding [@barbieri2005analysis]. 

Let's assume for the sake of argument that theta sequences are temporally aligned throughout the hippocampus, but different parts of the hippocampus preferentially participate in different parts of the theta sequence, with place cells in very lateral positions only firing in regions of the theta sequences that are two meters from the rat's current position. In this example, the place field of the rat would appear to be directly on top of the rat (this is how place fields are now defined), and two meters behind the part of the track that the place field is actually representing. The theta sequence that we decode will not extend two meters, because the definition we have used for place field will incorrectly attribute representation of the rat's current location to that lateral cell's spiking. Now, two-meter-long theta sequences have never been observed. Is this because they don't exist, or because we typically do not account for the possibility of prospective coding when we use the linear tracks that are optimized for recording large numbers of place cells? We don't know. It will be important and informative to try to address the issue of prospective coding in future studies of theta sequences.

The use of firing rate in the definition of theta sequences is problematic for a more general reason than the possibliity of prospective coding, though. Any spike that contributes to a theta sequence is a spike that will impact the shape of the place field's rate code as we currently define it (naively, the number of spikes fired as a function of the rat's current location). Implicit in the notion of a theta sequence is a separation between what a place cell encodes (we take this as the peak of the firing rate profile), and when the place cell expresses this encoding (a theta sequence is the unfolding in time of encoding of a sequence of locations on the track). Imagine for the sake of argument that a cell primarily interested in position x on the track parcicipate most heavily in theta sequences that extend from behind the animal to ahead. By construction we are only manipulating the ``when'' of encoding, but by the definition of the rate field, we can't avoid an effect on the ``what''. How much of a distorting effect does the coupling of theta sequenece firing have on our estimation of theta sequences?  We don't know, because although it may be appealing to remove the firing rate field from the definition of theta sequences (or vice versa), it is unclear what to replace it with.


* Real time position decoding from populations of place cells

** Abstract

Observational descriptions of hippocampal spatial encoding are outpacing our understanding of their underlying mechanisms and ties to behavior. The traditoinal manipulation techniques can not adequately target the richly choreographed spiking sequences increasingly recognized as an essential feature of spatial encoding. Some disruption specificity can be achieved by leveraging known statistical relationships between information content and the recency of spatial experience, and such experiments have provided the first evidence of a link between sequence replay and learning.  But this method stops short of being able to distinguish among the diverse forms of spatial content known to be expressed in a single recording session.

A method of decoding spatial information content in real-time is needed. To do this, we are developing a multi-tetrode recording system focused on streaming representations of the processing stages typically used for offline spatial decoding: spike detection, neural source separation (cluster-cutting), position tracking, tuning curve extraction, and Bayesian stimulus reconstruction. We also extend a method for position reconstruction without human spike-sorting to operate in realtime. Our implementation makes critical use of Haskell, a programming language that aides software development by strictly separating a program's logic from its effects on program state, greatly simplifying code and eliminating large classes of common software bugs.  We describe the capabilities and limits of our recording system, its implementation, and routes for contributers to add functionality; and we survey the classes of questions that could benefit from real-time stimulus reconstruction and feedback.

** Introduction

*** Theta sequences and replay in place cells, phenomenology
Temporally compressed spike sequences are increasingly recognized as an essential feature of hippocampal encoding of space. Each increase in our ability to sample large numbers of cells in freely navigating rats has been accompanied by further support this claim [@wilson1993dynamics; @miller2008all].

Physiologists are aware of two forms of sequential encoding. The first occurs during active navigation. The majority [@thompson1989place] of spiking activity in the hippocampus is due to place cells [@o1971hippocampus], which spike only when the rat is within an approximately 1 meter span of the track particular to that place cell (the cell's ``place field''). At any given time, the rat is within the (partially overlapping) place fields of many place cells. Rather than fire in random order, the spikes are arranged in precise sequences, with spikes from cells with place fields centered just behind the rat first, spikes from place fields centered ahead of the rat last, and a continuum between [@skaggs1996theta]. This sequence reflects the sequences of place field centers that the rat would encounter on the track, except it is sped up eight times and repeated once per cycle of the underlying 7-10 Hz ``theta'' oscillation in the local field potential [@dragoi2006temporal; @foster2007hippocampal]. 

A second form of sequenced spiking occurs while rats are paused on the track, consuming rewards or grooming. At these times, the hippocampus emits irregular, 100-500 ms bursts of local field potential ``sharp wave-ripples''(SW-R's) and spiking activity, with spikes ordered in time according to the spatial ordering of their respective place fields [@foster2006reverse; @diba2007forward]. These are known as 'sequence replay' events. Sequence replay often represents a track other than the track that the rat is currently running on [@karlsson2009awake]; indeed it was first observed in sleeping rats [@lee2002memory].



*** Summary of replay disruption studies
In contrast to the large number of studies exploring the phenomenology of theta sequences and sequence replay [@davidson2009hippocampal; @gupta2012segmentation; @karlsson2009awake; @pfeiffer2013hippocampal; @cei2014hippocampal], interventional studies are rare, because any specific activity pattern of interest is embedded in a network also exhibiting off-target sequences, and sequences themselves are not apparent to the experimenter without extensive post-processing. 

The content of sequence replay has a tendency to reflect recent experience, however. Some investigators using SW-R's as a trigger for immediate activity disruption have taken advantage of this to achieve some degree of stimulus selectivity in replay disruption. Ego-Stengel and Wilson [@ego2010disruption] and Girardeau et. al. [@girardeau2009selective] used this paradigm to show that selective disruption of sleep sequence replay of one track can delay the acquisition of a spatial task on that track, relative to another track. And Jadhav et. al. [@jadhav2012awake] disrupted all awake sequence replay and showed that this impacts working memory performance.

Using real time decoding, we could refine these experiments by disrupting only those replay events that correspond to the experimental portion of the maze, and leave replay of the control portion intact. This would provide more specificity in the question of whether relay is required for consodilation during sleep an working-memory performance.
      
*** Rationale for information-dependent replay manipulation

We would like to ask much more specific questions of sequence replay than whether or not it is needed for learning, of course [@lazebnik2004can]. Very fundamental things are still not known about replay. For instance, is its contents available to the animal for decision making? Are the contents under the rat's volitional control (as imagination is under humans' volitional control)? Definitive answers to these questions are hard to come by, but we could restrict the space of possibilities. By rewarding the rat for producing one type of replay and punishing him for producing another, an increase in production of the former by the rat would indicate that replay content is under the rat's control (although the mechanism of control may be very indirect). A lack of ability to adapt replay contents to the conditioning paradigm would suggest the opposite. In a complementary experiment, the experimental selection of a correct arm in a T maze could be determined by the rat's most recent replay before the trial - left-going replay will cause left to be the correct direction on the next trial, and vice versa. The ability to use this information or not gives us some evidence about the question of whether the rat is conscious of the content of his replay. Although in this case too, the consciousness may be of something incidentally correlated with replay content, rather than the content itself; so a lack of ability to learn a replay-guided behavior may be more informative than the positive result.

There are also some uses involving replay manipulation as more of a tool than a scientific question. For instance, we might like to test the hypothesis that replay events shape the properties of place cells on subsequent laps. If we have a means of encouraging an animal to produce large or small amounts of sequence replay for a given part of the track, then we have some experimental control over replay as an independent variable, and we can measure the subsequent effects of upregulating or downregulating it on place field shape.

Leaving the realm of sequence replay (but still considering ensemble stimulus reconstruction), these techniques could be useful for BMI applications.

*** Online replay decoding challenges

Position decoding has been used for several years as a means of summarizing the data of large numbers of place cells with multiple place fields [@davidson2009hippocampal; @karlsson2009awake; @zhang1998interpreting], and thanks to Zhang's report [@zhang1998interpreting] it is not a difficult analysis to do. But porting it to the real time context, where information is available in a streaming fashion instead of being presented all at once, presents some interesting and surprising challenges.

The first issue is /throughput/ - processing all of the data for time $x$ to $x+\delta$ must happen in less than $\delta$ time on average, or else a backlog of unfinished work will completely swamp the system. A related problem is /latency/ - even if the system has sufficient /throughput/ to keep up with the data stream, each computation step must finish with a small fixed offset from the time the data was acquired, if it is going to be useful for the experiment. The latency requirements for a behavioral feedback are generally lax, around 500ms, because we only need to detect a replay quickly enough to deliver some form of reward to the rat. Other experiments have much tighter latency requirements; interrupting an ongoing replay requires responses closer to 50 ms from the actual replay event.

Next we have to consider the /space complexity/ and /time complexity/ of the data structures and algorithms we choose [@hartmanis1965computational]. Different data structures have different advantages and disadvantages that are typically ignored in offline analysis. A classic example of this is the distinction between arrays and linked lists [@sedgewickalgorithms]. Arrays can be indexed in constant time (the time needed to look up up the n^{th} element does not depend on the size of the array) but do not support adding new values. On the other hand, linked lists allow appending elements in constant time, but indexing time is linearly proportional to the index. Data structures vary in the amount of space they take To cope with long-running experiments, we must avoid data structures that grow linearly with the number of spikes processed.

Finally there is the practical concern that different inputs are coming into the system at the same time, /concurrently/. Offline, we can ignore time and process the entirety of one electrode's signals at once, then iterate over the rest - that is of course not possible in real time ensemble-recording settings. In addition to the multiplicity of tetrodes, we have data additional concurrent data sources from the position tracking system and the input of the user. The process of decoding the data is conceptually concurrent from the incorporation of training data into the model. In general, concurrency and parallelism are the source of a large number of subtle bugs, and thus there is a great deal of active research into making concurrent computation more robust [@matsakis2014rust; @shavit1997software; @harris2005composable; @kuper2013lvars; @jones2001tackling].


*** Minimizing human intervention: no time for manual spike sorting
The most labor-intensive part of the post-processing involves sorting the multi-unit spiking activity recorded on each tetrode into the single-unit spike trains of putative single neurons. It is often impractical to manually segment many tetrodes' spikes into putative single units, especially during a realtime experiment, when clusters need to be cut before any realtime feedback can be administered. 

Kloosterman et. al. [@kloosterman2014bayesian] developed a method for Bayesian stimulus decoding from tetrode data without explicit spike sorting and provided an implementation in MATLAB. This implementation is only suitable for offline position due to the use of alrogithms that take time proportional to the number of processed spikes, and the poor performance characteristics of MATLAB. But we can address these issues by reimplementing the idea using different data structures and alrogithms, in a language with good concurrent programming support.


*** A proof of concept in c and Haskell
Here we report on two advances toward this goal. The first is a new system for simple acquisition, bandpass filtering, and multi-unit spike detection capable of running in tandem with our existing recording systems. The second is a proof-of-concept application that streams raw spike data and rat position data from the hard disk, performs source separation based on previously-determined waveform discrimation criteria, builds place field models, and performs the Bayesian inference to reveal sequence encoding, all in realtime.

The data acquisition system was written in a mix of c++ and Python, where signal processing and networking can be done using common libraries within grasp for beginners (which we were at the time). The realtime decoding system presented more interesting challenges, in terms modeling place fields, supporting infinite data streams, and concurrency. For this system, we turned to Haskell [@jones2003haskell], a language optimized for ease of building composable abstractions [@hughes1989functional], through the marriage of a highly extensible static type system and functional purity. Haskell's type system enables the programmer to build custom times that capture the much of the intent of a model or algorithm, allowing the large classes of bugs to be eliminated by the compiler. Functional purity is an engineering discipline strictly enforced by Haskell that forbids variables from changing their values during program execution. This restriction, thought apparently limiting, has many highly favorable consequences for managing complexity. These features fit together exceptionally well for designing highly concurrent programs, a notoriously difficult task in all programming languages [@jones2001tackling; @harris2005composable].

Our application currently reads spikes in multiple files at the rate they were initially recorded, passes them through previously-determined cluster boundaries, combines them with a record of the rat's position also stored in a file, and produces a stream of place fields and a composite visualization of the rat's position in real time. As we develop the application, it will be able to interface with the system performing the real time recording, track the rat in real time, accomodate stimuli other than spatial location, and sort spikes into single units without manual cluster-cutting.

** Materials and Methods

*** Backend signal acquisition and networking
Raw data is aquired simultaneously, at 32kHz, from 32 channels simultaneously on 2 NI PCI-6259 analog-to-digital converter cards (National Instruments), using the NIDaqMX c API. After passing data from the driver's memory to our program, samples are written into a circular buffer and passed through a 4th order Butterworth IIR filter. This choice of filter requires only two samples of history per channel, imposing a very short delay (< 1ms) between the collection of a given sample and subsequent processing. Spikes are detected by comparing each sample to a threshold, noting threshold crossings, and then waiting for one or a few cycles of acquisition until enough samples have been collected to meet the waveform length required by the user. Parameters like filter properties, spike threshold, and spike waveform length are initially set in a configuration file, and later modified through a networked API, so that the program can be run without an immediate graphical user interface - this is a preferable arrangement for a parallel, potentially distributed system, in which we may want a single command issued by the user to affect recording systems running on multiple computers.

Our previous recording system (AD. M.A.W. 1998 TODO date, cite?) also ran as a distributed collection of low-end acquisition computers receiving analog signals as input. In order to compare the recording quality and timing of our new system to the old system, we physically split sets of four analog inputs to two separat amplifiers - one serving each recording system. AD relies on hardware filtering of broadband data into the spike waveform band (300-6000 Hz) by a 3rd order Butterworth filter. ArtE reduces the hardware system requirements by digitally filtering a single broadband input into two signal bands - the spike band and the local field potential band (0.1 - 475 Hz), in each case using a digital filter designed to mirror the properites of AD's analog filters. Finally, using both systems in tandem required careful timebase coordination. Using standard computer system clocks is completely inadequate, as network delays between computers are on the order of several miliseconds, and can vary depending on system load. Instead, we route a diginal clock signal used to synchronize the AD computers into the ArtE system, and manually issue a counter resetting command to ArtE over the network while AD does the same for its own synchroization process. This fairly hard-coded timebase integration is one problem that will have to be solved before ArtE can be used in isolatoin from AD, but not a very difficult one.

Isolated spike waveforms as well as downsampled, continuous local field potential signals are saved to disk in a different format from the one used in the rest of our cluster-cutting and analysis workflow. Until these tools are rewritten to work with the ArtE data format, we convert ArtE files into AD format, and continue with xclust (MAW - TODO) for cluster-cutting and MATLAB (Mathworks, Natick, MA TODO) for general analysis.

*** Offline position reconstruction
We compute fast timescale summaries of neural ensemble activicty through Bayesian stimulus decoding, as described in Zhang et. al. \cite{zhang1998interpreting}. Implementations of this procedure to date, including those used in our lab \cite{davidson2009hippocampal} are decidedly unfriendly to streaming, as they build models of place fields by sorting all spikes from the beginning of the recording session into the spatial bins partioning the track. This operation has time and space complexity linear in the number of recorded spikes, making it unsuitable for continuous streaming. Place field computations derived late in the recording would take longer than those computed at the beginning, and memory would be exhausted in finite time. These problems do not interfere with offline position decoding, because place fields may be computed once,slowly, and used repeatedly. The computation of many place fields that are synthesized into a single position estimate may be computed serially.

*** Online position reconstruction

-  Manual spike sorting probably far too slow, use semi-automated or
   clusterless
-  Choosing data structure for spike sorting & decoding with bounded
   memory & time use
-  Likelihood functions have to be updated during experiment

   -  By a lot of threads (~ 32 tetrodes * spike rate, plus current
      position)
   -  Decoder also writes to likelihood function

-  Use Haskell's concurrency library to coordinate many writing/reading
   threads


Modifying the place field models to update in constant time, rather than performing a linear-time recomputation for each incoming spike, is straightforward. Treatment of a large number of such models in parallel, rather than serially, is more challenging, because these models are ultimately combined into a single position estimate. Aditionally, the process of model update must run concurrently with graphic renderings, user input, and the regular computation of the position estimate itself.

To perform Bayesian decoding in realtime, we left the relative comfort of c++ and MATLAB for Haskell, on the promise that Haskell's type system and functional purity guarentees would simplify the static design of the model, and aid in the highly concurrent data flow. 

*** Modeling place fields with Haskell data types

The phenomenology of place fields and the diversity of maze environments add complexity to the core notion of computing the place field, which is simply spike rate as a function of track position. These complexities are generally addressed in an ad-hoc way appropriate to each experiment. Due to the increased engineering effort involved in performing reconstruction in realtime, we aimed to anticipate as many of these issues as possible in the design of our stimulus model. We specify mazes as a collection of spatial bins, each with a user-assigned ``outbound'' direction and physical width. An animal's relationship to the environment is thus the combination of its relationship two each spatial bin in three respects, (1) physical proximity to the bin, (2) ``outbound'' or ``inbound'' heading with respect to the bin, and (3) position of the head with respect to the track width, either ``centered'' or ``leaning over''.

Matrix-based languages like MATLAB and c would suggest a representation of a place field as a three-dimentional array (with bin identity in the first dimension, the two possible heading directions in the second dimension, and head-overhang in the third dimension, for example). A particular position is referenced as an index into that array (for instance, the value at field[14,1,2] could correspond to a stored value related to the 14th spatial bin, inbound running direction, head overhanging the edge). This is error prone. It requires the programmer to remember the mapping between maxrix dimenison and stimulus dimension, as well as a mapping between discreet values and stimulus levels (for example, than 1 means ``inbound'' and 2 means ``outbound''). Naming the levels with variables does not solve the problem, because the variable ``outboundLevel'' and ``headOverhanging'' are both of the same type. Accidentally swapping the two (for example, writing field[14, headOverhanging, outboundDir] (TODO code formatting)) will result in code that compiles and runs, but produces incorrect output.

Haskell idioms are much safer. Instead of indexing into a matrix using three Integers, an idiomatic Haskell solution would be to use a tripple of indices with different types as the addressable space over which occupancy or a place field is defined. The use of distinct types for bin, direction, and alignment 'indices' allows the compiler to check the work of the programmer at every point where indexing happens. This small difference in approach eliminates a very large fraction of the bugs a codebase acquires as it changes and incorporates new features over time. If the matrix dimensionality were to change to accomodate a new feature, the Haskell compiler would enforce that this change is accounted for at every point where the code tries to access the matrix. This is in stark contrast to the flexible addressing of MATLAB and the untyped addressing of c/c++ arrays - in both of these cases the change may not result in any complaint from the program, but will instead happily deliver either noisy (or worse, unnaturally structured) data.

Our Haskell model of the track is the basis for the model of the rat's instantaneous ``position'', the model of accumulated time spend at each position (the ``occupancy'' function), and the model of a place field. At each point in time, we compute the animal's ``position'' as its relationship to each bin. In the simplest case, the bin that the rat occupies is given a score of 1.0, and all other bins scored 0.0; more typically, we assign graded scores to the bins according to their proximity to the rat; this method is favorable for smoothing noise in place field computations. For those time bins when the animal is running, this instantaneous position function added to an running tally of time spent at each position (``occupancy''). 

A place field is modeled in a similar mannar to the occupancy map - as a function from spatial bin to a number roughly equivalent to a ``spike count'' in that bin. Each time a neuron fires a spike, the instantaneous position map is added to the place field function accumulated so far. In the simple case when the spatial bin containing the animal is assigned a 1.0, each spike adds an integer to that spatial bin in the place field. When position is taken by the more usual Gaussain-smoothed method, each spike adds a gaussian curve to the accumulated field. This procedure gives us constant-time, constant-memory spike-count functions that are simple to update, while respecting the complexity of the underlying behavior (the separate consideration for outbound vs. inbound running direction, and the consideration of whether the head is aligned with the track or leaning over the edge). When needed, the actual firing rate function can be computed, in constant time, by dividing the neuron's specific spike-rate function by the global occupancy function, at each spatial bin.

*** Managing concurrency and data streaming

To decode in realtime, we must simultaneously update place fields with information from new spikes, update the current postion of the rat, read the place fields and combine them into a single position estimate, handle user input, and render something to the screen. All of these operations interact with the same underlying data, and thus the problem is inherrently in a difficult programming regine (TODO CITE Concurrency difficulty paper). Due to strict enforcement of functional purity and immutable data, Haskell is in a special position to simplify concurrent computations. Indeed, the STM library provides a lockless concurrency scheme that allows multiple threads to simultaneously modify the same data if they wish (this genarally leads to data corruption), as long as the only variables modified are of a special type provisioned by the library, called TVars. STM tracks access to these veriables, detects when two threads have made conflicting changes, and roles both changes back, allowing the threads to attempt their modifications again.

We took advantage of the STM library to coordinate this concurrent read and write access to a single state value. This value was stored in one large TVar, which could be updated in the infrequent event of user input or the addition of new tetrodes. Within the enclosing state value, each place field is stored in its own TVar. In this scheme a very large number of spikes can be distributed to their respective place fields, and updates can be made without regard for the activity of other place field updates.

The problem is not amenable to processing by entirely independent threads (``embarassingly parallel''), because the decoding step requires access to all place fields. In addition to place field updates, we accumulate spike-counts within short timewindows, and the decoding thread must reset all of these counts to zero each time a position estimate is produced. We group the resetting of all place field cell counts into a single atomic operation, to prevent the data inconsistencies that would inevitably arise if count-updating and count-resetting were interleaved. The grouping of actions into atomic blocks that can be retried upon collission is precisely the strength of the STM library that makes it so suitable for the structure of our decoding algorithm.

*** Clusterless decoding
We extended the clusterless decoding method of Kloosterman et. al. [@kloosterman2014bayesian] by providing a new implementation that runs in bounded memory and time (Kloosterman's takes time and memory proportional to the number of spikes recorded, which makes it too slow for large-scale, long-running recordings). To restructure the alrogithm in a way that would continue to perform with potentially-infinite streams of data, we turned again to Haskell for its ease of use when working with custom data structures.

Kloosterman et. al.'s algorithm requires the comparison of recently-received spikes (the testing-set) to the amplitudes of all spikes received from the beginning of reconding (the training-set) along with the rat's track location during those training-set spikes. An estimate of the rat's position at testing-time is derived through Bayesian inference over a combination of the training-set spikes weighted by their amplitude-similarity to the testing-set spikes. A literal implementation of this algorithm has the disadvantage of making a larger and larger number of comparisons as the experiment progresses and the training-set grows. An obvious alternative would be to divide the space of spike amplitudes into a set of cubes, and update the cube into which each training-spike falls with the rat's current position. However, because amplitude space is four dimensional, the number of cubes required to tile amplitude space at a reasonable resulution is too large to store in computer memory. Sparse matricies and KD-trees are two good data structures for holding multi-dimensional data in limited memory. We chose the reimplement clusterless decoding using the latter, at a slight performance penalty, because trees are somewhat more convenient to work with than matricies in Haskell. In order to accomotade new training-set spikes in bounded memory, when a new spike arrives less than some threshold distance from its nearest neighbor, the two are combined into one, and the payloads of the two (the place fields) are summed according to each point's weight. 

** Results

The results of our effort to date are a working real time decoding alrogithm and a proof-of-concept system of supporting infrastructure. The core alrogithm takes advantage of Haskell's highly efficient runtime system and composable concurrency model to combine spiking and positional information in real time and produce a streaming Bayesian estimate of the rat's location. The bandwidth of the system is sufficient for decoding fast-timescale features like theta sequences and sequence replay. The development process itself made critical use of Haskell's type system features, which drastically improve the programmer's ability to reorginaze code and discover logical and typographical errors at the time of program compilation.

*** Decoding fast-timescale features: theta sequences and replay

Simply decoding the rat's position is a potentially useful engineering goal, but in general the rat's instantaneous position is more conveniently estimated using an overhead camera. The features we are really interested in gaining real time access to are those internal states that deviate from the rat's physical location; and these deviations happen on a very fast timescale - the timescale of theta sequences and sequence replay. Thus one of our primary design goals was to achive a processing bandwidth capable of estimating the rat's position in 20ms time windows. Figure 7 shows a position estimate computed post-hoc (top row), and the estimate derived from real time processing of the same data, at a lower spatial resolution (bottom row). The data set used included 33 place cells recorded on 8 tetrodes. This is combination of spatial resolution and cell count was near the processing limit for our machine, although we expect that the bandwith will increase substantially when the various cell-sorting tasks are split among multiple computers, as they would be in a full recording system. The middle and right pannels show decoded theta sequences and sequence replay respectively.

#+NAME: fig:decodingExamples
#+CAPTION: \textbf{Fast time-scale position decoding.} Reconstructed features computed in real time by the ArtE decoder (bottom row) match those derived post-hoc (top row). The features are harder to resolve in the ArtE case because we decoding position at a courser spatial scale (20 cm bins /vs./ 3.5 cm bins), but are still sufficient for the detection of events that would be used as triggers in a closed-loop experiment. Theta sequences (middle) and sequence replay (right) are both recoverable from a typical place cell population in real time.
[[./finalFigs/headToHeadDecoding.png]]

*** Decoding speed and realtime requeriments

#+CAPTION: \textbf{A timing failure mode.} 
#+NAME:   fig:arteTiming
[[./finalFigs/arteTiming.png]]


*** Bugs, deadlocks, crashes and performance debugging

#+CAPTION: \textbf{Memory profiling helps performance debugging.} Most Haskel debugging is performance debugging, because it can be hard to see which parts of a program will accumulate resources over time. The GHC Haskell compiler produced this plot of memory usage over time broken down by module. The runtime system uses the most memory (green). The Histogram module (yellow), which is part of the ArtE project, fails to release data and grows linearly with time. These profiles generally make it easy to find the code errors that lead to ramping memory use and program slowdowns.
#+NAME:   fig:arteDecodeProfile
[[./finalFigs/arteDecodeProfile.png]]


** Discussion

*** A tool for decoding streaming place cell data
We have developed a working algorithm and a proof-of-concept test system for performing Bayesian stimulus decoding [@zhang1998interpreting; @davidson2009hippocampal] on populations of place cells in real time. The implementation makes critical use of Haskell's type system, extensive open source libraries [@jones2003haskell], fast runtime [@peyton1987implementation; @gill1993short; @voellmy2013mio], and concurrency and parallelism support [@jones1996concurrent; @marlow2009runtime; @marlow2012parallel] (specifically, using the GHC Haskell compiler [@jones2003haskell]).

As other users of Haskell have noted, one difficulty in writing fast code is avoiding patterns that interact poorly with Haskell's lazy evaluation symantics [@daniels2012experience]. This was a stumbling block in our implementation as well, as some mistakes lead to slowdowns that become evident only after several minutes of running, and are therefore difficult to eliminate by trial and error. Fortunately the GHC compiler provides tools for tracking memory and time usage; these tools greatly aided our performance debugging experience.

*** Remaining components needed to run experiments
Several components need to be built before our decoding system can be used in closed-loop experiments:

  - /Tracker/: Rat tracking software capable of linearizing twisted tracks and transforming 3D position into 1D track location and heading direction.
  - /Replay discriminator/: A means of deciding from the stream of decoded positions, when a pattern counts as /replay/. This must be customizable enough to fit different experimental demands, such as conditioning feedback on a replay's virtual run direction on a T-maze.
  - /Networked decoding/: A means of splitting the work of model training and model testing across multiple computers, to remove the CPU decoding bottleneck and allow the use of more than eight tetrodes
  - /Network transport/: A common protocol for packaging various types of data (animal location, neuron spikes, intermediate decoding data, behavioral sensor values, maze actuator commands, etc). Ad-hoc networking is not quite sufficcient because many of our components need to fan out to multiple listeners (e.g. a spike source) or fan in from many sources (e.g. the stimulus decoder).

We plan to develop these components ourselves in same way that we developed the ArtE backend system, by replacing one component at a time into the existing AD recording system built by Matthew Wilson and Loren Frank, and doing integration tests comparing AD's native output to the output with one adapted component. Although there is a lot of work left to do before the system can do end-to-end work, it's not too early to start fantasizing about the experimental possibilities. We describe some of these in the next section.

*** Possible experimentals using real time decoding
An important set of experiments to do is to refine the ripple disrpution studies [@ego2010disruption; @girardeau2009selective; @jadhav2012awake]. The goal of these studies was to determine whether or not sequence replay is necessary for memory consolidation (in the case of [@ego2010disruption; @girardeau2009selective]) or working memory (in the case of [@jadhav2012awake]). Jadhav et. al. disrupted all ripple-related activity on the track. There was no attempt made to restrict ripple disruption to the ripples carrying one type of replay or another[fn:: In fact, their control condition was to begin inhibition 200ms after ripple detection, with the intention of finding a disruption scheme with similar timing characteristics to ripples and avoiding times when the animal is running, without blocking ripples themselves. As Davidson et. al. [@davidson2009hippocampal] and Layton and Wilson [@layton2013temporal] showed, some replay events are longer than 200 ms, and if these were present in the Jadhav study, they may have been truncated.]. They found that this disruption interferes with the rat's ability to choose a maze turn direction based on recent memory, but does not interfere with maze choices that have only long-term memory requirements. But they could not tell, for example, whether individual replay events carry the short-term memory trace used by the rats on individual trials, or whether replay disruption is generally upsetting to performance of the more difficult phase of the task. Indeed, task difficulty is a factor that often distinguishes between the experimental and control phases of a behavioral task[@beylin2001role], but it is not often acknowledged as one. With online replay content detection, the two equally difficult working-memory tasks of the Jadhav study could have been made controls for one another, by selectively disrupting replay corresponding to one of them.

Ego-Stengle and Wilson [@ego2010disruption] and Gerardeau et. al. [@girardeau2009selective] used the fact that replay of recently experienced tracks is more common during sleep than replay of tracks learned several days ago in order to selectively disrupt the replay of one track over another. In experimental designs like these, having the ability to single out one type of replay for disruption would further refine the selectivity, perhaps enhancing the differential effects on behavior.

Real time replay detection provides oppornunities for experimental designs that weren't possible before. Fewer than half of replay events during wake contain decodable spatial content pertaining to the current maze [@davidson2009hippocampal], and the fraction in sleep is much smaller [@lee2002memory; @ji2006coordinated]. If we want to make reward or task contingency conditional on replay, then decoding replay content in real time is a hard requirement.

For example, we may want to test whether left-going replay on a T-maze[fn:: Alternatively, some behavioral factor correlated with replay!] can be used as the behavioral response that is rewarded or punished to produce operant conditioning, in a specific way that doesn't generalize to right-going replay, to test whether the content of replay or a behavioral correlate of that content is something cognitively available to the rat. 

There is currently very little evidence for a one-to-one connection between replay content and immediate past or present behavior [@davidson2009hippocampal; @gupta2010hippocampal; @pfeiffer2013hippocampal]. The dual of the operant conditioning experiment would be to try to train a rat to recognize that, whichever direction his most recent replay took, that is the direction he must run next to find a reward.

A third class of experiments is nebulous but probably very valuable. Using real time Bayesian position decoding (not necessarily replay detection per-se), an experimenter would have immediate access to the joint activity of the recorded population of place cells. Many classical discoveries in neuroscience are due to chance observations rather than premeditated binary-choice hypothesis-based predictions. The discover of place cells themselves [@o1971hippocampus] is due to tinkering with a rat while listening to the audio aplified spiking of an individual hippocampal neuron; as was the discovery of the primary visual cortex simple receptive field [@hubel1959receptive] and the large litterature that followed [fn:: Oriented moving bars of light were famously discovered to be the optimal stimulus for driving spiking in the cells of primary visual cortex when David Hubel and Torsten Wiesel were changing slides in a projector slide deck; although the many pictures of animals and natural stimuli on the slides failed to elicit a responce, the sweeping motion of the edge of the slide as slides were being changed in and out of the machine caused very robust spiking. Hubel and Wiesel creatively pointed their projector at a chalk board, systematically moving the slide edge and making chalk marks at the edge locations and orientations that best excited the cell nearest the electrode. This approach is what we mean by using real time feedback for 'tinkering'.] We expect that when decoded position is available to the experimenter, creativity in the moment will lead to informal experimentation that sheds light on the nature of replay in a way that would not be possible with the traditional, slow data collection / data analysis cycle.



* Retrosplenial slow-wave wake and interaction with hippocampus

** Introduction

*** Cortico-hippocampal sleep interactions, possible role in memory
Both cortex and hippocampus exhibit interesting forms of structured activity in sleeping animals. In the hippocampus, a brain rhythm known as a sharp-wave ripple is known to carry information about sequences of locations on a recently visited track [@lee2002memory; @foster2006reverse; @diba2007forward; @davidson2009hippocampal]. The information-richness of these coordinated activity patterns makes us hopeful that there are inroads into understanding the encoding of information and its mechanism of storage. It is thought that cortex has a role in long term information storage, and that sleep is as an important time for memory formation [@wagner2004sleep; @gais2002learning; @marshall2006boosting; @stickgold2001sleep]. The interactions between hippocampal activity and cortical activity may provide clues about the role of sequence replay in the rest of the brain, the mechanism of information transfer from hippocampus to cortex, and the mechanisms of that information's long-term storage.



*** Slow wave oscillations cleanly destinguish between sleeping and awake cortex
The most striking feature of cortical activity in the sleeping brain is a pattern known as /up-down states/, the /delta rhythm/, or /frames/, depending on the recording method [fn:: /Up-down states/ refers to intracellular recordings, /Delta oscillations/ to EEG + LFP, and /frames/ to extracellular multi-unit recording respectively. We use the term frames where the distinction is not important]. They are three views of the same underlying phenomenon: the coordinated switching between an online, wake-like state with spikes, and a hyperpolarized state profoundly devoid of spikes.

This pattern is not seen while an animal is awake. It has so far only been observed in animals that are drowsey[@vyazovskiy2011local] or in slow-wave sleep[@steriade1993thalamocortical], one of the two primary sleep stages. The other sleep stage, REM sleep. [fn:: REM stands for /Rapid eye movement/. REM sleep is accompanied by movements of the eyes that resemble awake visual exploration [@aserinsky1953regularly; but see @aserinsky1985comparison].] REM sleep is also referred to as /paradoxical sleep/, because of the similarity of REM brain activity to activity patterns in awake animals.

*** Theta and ripples destinguish between 'online' and 'offline' hippocampus
The hippocampus is similar to cortex in that it has two very different modes of operation, but they are not as strictly linked to sleep and wake as the activity patterns in cortex are. Instead, they reflect the 'online' or 'offline' nature of attention. During visual exploration, running, foraging, etc. (and during REM sleep) the hippocampus and many of its input and output structures are engaged in an 7-10 Hz oscillation called the /theta rhythm/ [@vanderwolf1969hippocampal]. When an animal's attention is directed inward and while the animal is in slow-wave sleep, theta ceases and is replaced by /Large irregular activiry/, a generally quiet state interrupted by ~50ms bouts of coordinated vigorous spiking[@ylinen1995sharp], sometimes grouped into /ripple bursts/ that last half a second [@layton2013temporal].


*** Retrosplenial cortex unexpectedly follows HPC into SWS-like state during reward
One popular model proposes that memory formation goes in two stages: while animals are awake, sensory information flows through the cortex and into the hippocampus, where it is assembled into short-lived mental models of the various sorts of things to be remembered; and while they are sleeping the hippocampus projects a modelled form of this information back to the cortex, with repetition, for long-term storage [@buzsaki1989two]. This model serves as a backdrop for designing experiments that examine the effect that concrene hippocampal events have on cortex, and that concrete cortical events have on hippocampus.


** Results

*** Characterizing slow-wave sleep (SWS) in cortex

-  Examples of light sleep, spindles, frames and K-complexes in LFP,
   spiking
-  Examples of deep sleep, frames and K-complexes in LFP, spiking
-  Distribution of activity over all cortical electrodes
-  Average up-state length, down-state length

*** Retrosplenial cortex enters SWS-like state during novelty / large rewards

We trained rats to run clockwise around a 3.4 meter circumferenec track to receive food reward (Figure 11), in order to engage navigational circuits in the hippocampus as well as head-direction circuits in retrosplenial cortex. Most rewards were small 5g bolus of wetted powdered rat chow, delivered at a single point on the track (requiring a full lap for delivery) for three laps. On subsequent laps, we rewarded the rat once for each 270Â° degrees of track running (for reasons not related to this study - we were reusing the task design of a study we were aiming to replicate about REM replay [@louie2001temporally]). Once in every 4 to 6 trials, we instead used a 20g bolus, to encourage the rat to stop, eat, and produce more ripples and replay events. We positioned electrodes in hippocampus and retrosplenial cortex and attempted to optimize for isolated single units. In later phases of the experiment, we moved a number of electrodes up from hippocampus into somatosensory and motor areas, to explore further the phenomenon we noticed in retrosplenial cortex. During track running and sleep, we often listened to the amplified activity of either hippocampus or cortex (switching back and forth over the course of 30 minute sessions).

#+CAPTION: \textbf{Recording sites and behavioral training.} /Top:/ Behavioral training was carried out every day over the course of the recording and consisted of clockwise running on a 3.4 meter circular track. For the first the rat got normal (5g TODO) rewards. For the remainder of the session (usually 10 to 20 laps) normal reward was delivered every 270Â°, but occasional nomral rewards were replaced with large (25g TODO) rewards, to encourage longer pauses and more hippocampal replay.
#+NAME:   fig:expDesign
[[./finalFigs/SWW/expDesign.png]]

We became acustomed to the sound of ripples and spindles in the multi-unit spiking activity of cortex in the sleeping rat. We were surprised to hear a subjectively similar pattern from retrosplenial cortex while rats stopped to eat large rewards. Comparing the raster plots and LFPs of several, retrosplenial cortex activty transiently but strongly resembles the structure of activity during slow-wave sleep (Figure 12), with characteristic sharp breaks in ongoing activity and accompanying K-complex like LFP oscillations. In another set of experiments, conducted at the end of a 5-hour period of sustained wake in the middle of the day, we recorded activity during foraging in an open field, using novel objects to keep the rat in a curious state and preventing him from sleeping on the maze. During these periods of drowsiness, we saw similar interruptions in ongoing retrosplenial cortex spiking.

#+CAPTION: \textbf{Example frames in SWS, drowsiness, and large reward consumption.} Example frames and down states are clearly visible and coordinated between retrosplenial cortex (green) and somatosensory and motor cortices (magenta) during SWS. Down states are less frequent during drowsiness and reward consumption. In drowsiness, weak coorditation between cortical areas can be seen. During awake reward consumption, frames and down states are visible in retrosplenial cortex, but somatosensory and motor cortex fire as they normally do during wake.
#+NAME:   fig:frameExamples
[[./finalFigs/SWW/finding.png]]

In the retrosplenial cortex, lengths of putative down states in slow-wave sleep, drowsiness, and consumption of large reward were distributed similarly (Figure 13). The same is true of the scattered somatosensory and motor cortex electrodes, with the exception that thees only exhibited down-states during slow-wave sleep and drowsiness. No down states meeting our criterion of 5 milliseconds at less than 40 Hz multiunit activity were observed.

#+CAPTION: \textbf{Distribution of down-state lengths in SWS, drowsiness, and large reward consumption.} Down states were defined as intervals with mean spike rate per tetrode below 40 Hz lasting for at least least 5 ms. Down states in retrosplenial cortex and somatosensory cortex are similarly distributed. The distribution appears during large reward consemption periods in retrosplenial cortex, while no awake down states were seen in somatosensory and motor cortex.
#+NAME:   fig:downStateDistributions
[[./finalFigs/SWW/downStateLengthDistributions.png]]


*** RSC awake slow waves coordinate with hippocampal ripples

-  5-second window showing co-transition into SWS-like state (RSC
   frames, HPC ripples & replay)
-  200-second window showing behavioral-timescale relationship
-  Cross-correlation of ripples & RSC frames similar between wake and
   SWS

*** RSC awake slow waves require large reward in well-trained rats9

-  Occur at most stopping points early in training
-  After ~ week, spontaneous frames & small-reward frames stop, but
   large-reward frames persist (for at least a month)

*** Anatomical restriction - nonparticipation in other cortical areas

-  Simultaneously recorded somatosensory, motor, posterior parietal
   cortex have no frame-like activity (noticeable changes in spike rate
   or LFP) during RSC awake frames


*** Slow-wave wake not limited to times of sleepiness (TODO maybe include maybe not)

-  Awake SWS-like activity continues in both light and dark phases of
   light cycle
-  Many SW's are flanked by fast running and chewing

** Discussion

*** Recap: Awake slow-waves in RSC, coordinated with HPC, fully awake
We show that in retrosplenial cortex, slow-wave sleep like activity can be reliably elicited in the fully awake rat using large rewards, and this activity is coordinated on the behavioral and neural timescale with the /offline-mode/ of activity in hippocampus. 

These results are similar to the /local sleep/ findings of Vyazovski et. al. [@vyazovskiy2011local], but the primary difference is that the results we report seem to be unrelated to drowsiness, while Vyazovski's /local sleep/ is strogly related to sleep need and momentary behavioral impairment. Our findings suggest something else - that local sleep may be a normal part of the waking state, and that the hippocampal-cortical dialog previously thought to occur only during sleep may also occur when large rewards are found.

*** Functional roles for HPC-Cortex coordination may apply to wake


*** New questions raised by SWW: mechanism and function

-  New questions:

   -  What other brain areas have SWWake? Papez circuit?
   -  What's the mechanism for the switch from awake-aroused to SWW
      cortex?
   -  What causes Slow Waves to traverse all of cortex during sleep, and
      not wake?
   -  Is there information content in slow-wave frame spikes? Is it
      bounded by slow wave boundaries in an interesting way?

\pagebreak

** Materials & Methods
*** Subjects

All procedures were approved by the Committee on Animal Care at Massachusetts Institute of Technology and followed US National Institutes of Health guidelines. Tetrode arrays were assembled and implanted  according to the procedure in Nguyen et. at. (TODO 2008) and Kloosterman et. al (TODO 2008). We made several modifications to the materials and proceduces to improve our multi-cell sampling.  First, we glued several hundred half-inch pieces of 29 guage and 30 guage hypodermic tubing into rows about 6 mm long, then stacked and glued the rows together to form a honeycomb patterned jig, for organizing the tetrode guide-tubes that would eventually inhabit the microdrive. Second, we developed the ArtE recording system (TODO detailed in Chapter 2) to run in parallel with our usual usual tetrode recording rig. The broader goals of the ArtE project are to enable real-time data analysis and feedback, but in this experiment we used it merely to increase the number of simultaneously recorded tetrodes.



*** Single-unit tetrode recording

Microdrive arrays were implanted with the center of the grid of tetrodes overlying dorsal CA1 (TODO A/P -4.0, M/L 3.5), spanning 3 mm of hippocampus in the septotemporal dimension and 1.5 mm proximo-distal. In two rats (TODO correct?), tetrodes were lowered into the pyramidal cell layer of CA1 over the course of 2 to 3 weeks and left there for several more weeks of recording.  In two more rats, tetrodes were first lowered into CA1, and later a subset of those was moved further to record simultaneously from field CA3. In each cell layer, we sought to maximize the number of neurons recorded and to minimize within-experiment drift, so closely tracked the shape of sharp wave ripples (which undergo characteristic changes during approach to the cell layer) and later the amplitudes of burgeoning clusters. If either of these factors changed overnight to a degree greater than expected, the tetrode was retracted by 30 - 60 micrometers.

*** Behavioral training

Behavioral training consisted of rewarding rats for simply running back and forth on a curved 3.4 meter linear track, or running continuously clockwise on a 3.4 meter long circular track, with rewards given for every 360 degrees of running for the first 3 laps and for every 270 degrees thereafter. Food deprivation began one or two days prior to the beginning of acquisition, with rats receiving 30 grams of food per day, adjusted up or down depending on the rat's motivation to run and level of comfort (assessed by the amount sleep taken before the running session). The target food-deprived weight was 80\% of free-feeding weight, but we rarely achieved this without disrupting the sleep of the animals, so body weights tended to be 90\% of the free-feeding weight or more, especially after rats learned the simple rules of the task. Additionally, we provided large rewards throughout training (2-5 grams of wetted powdered rat chow per lap), to encourage the long stopping periods during which awake replay can be observed (TODO Foster, 2006). Under these conditions, rats run for about 20 laps or 30 minutes before becoming satiated and ignoring rewards.

*** Electrophysiological Characterization
Spikes and local field potentials were voltage buffered and recorded against a common white-matter reference, at 32 kHz and 2kHz respectively, and position was tracked at 15 Hz through a pair of alternating LED's mounted on the headstage, as in (TODO) Davidson et. al. (2009). Spikes were clustered manually using the custom program, xclust3 (M.A.W. TODO). Place fields were computed for each neuron as in Brown Sejnowski et al (TODO), by partitioning the track into 50 to 100 spatial bins, and dividing the number of spikes occurring with the rat in each spatial bin by the amount of time spent in that spatial bin, in each case only counting events when the rat was moving at least 10 cm/second around the track. Direction of running was also taken into account, allowing us to compute separate tuning curves for the two directions of running, which we label 'outbound' and 'inbound'.

To characterize the phase differences among tetrodes in CA1, a simple spatial traveling wave model was fit to the theta-frequency filtered LFP signals and the theta-filtered multiunit firing rate in turn, as in Lubenov and Spapas [@lubenov2009hippocampal]. TODO expand on this.



* Conclusion / Wrap-up

Brief summary of the role of populations of neurons in hippocampal
spatial coding. Much more reliability in the timing of place cell spike
sequences than there is in single cell measures like phase precession.
We want to know if population sequences are an essential feature of
coding, or just a means of denoising, and answering that question will
involve manipulations that account for information content in and react
to it in real time, as well as studies of how population sequences are
interpreted by downstream cortical areas.



* References
